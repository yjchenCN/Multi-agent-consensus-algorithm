{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T08:22:56.837394Z",
     "start_time": "2024-07-23T08:18:52.390448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 30/30 [00:02<00:00, 12.84it/s, episode=30, returns=-27746.030]\n",
      "Iteration 1: 100%|██████████| 30/30 [00:02<00:00, 13.84it/s, episode=60, returns=-25069.699]\n",
      "Iteration 2: 100%|██████████| 30/30 [00:02<00:00, 14.38it/s, episode=90, returns=-25743.452]\n",
      "Iteration 3: 100%|██████████| 30/30 [00:04<00:00,  6.16it/s, episode=120, returns=-28831.311]\n",
      "Iteration 4: 100%|██████████| 30/30 [00:06<00:00,  4.80it/s, episode=150, returns=-22808.536]\n",
      "Iteration 5: 100%|██████████| 30/30 [00:06<00:00,  4.79it/s, episode=180, returns=-25196.661]\n",
      "Iteration 6:  20%|██        | 6/30 [00:01<00:05,  4.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 387\u001b[0m\n\u001b[1;32m    385\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    386\u001b[0m qmix \u001b[38;5;241m=\u001b[39m QMIX(env, args, use_cuda)\n\u001b[0;32m--> 387\u001b[0m total_q_values, total_returns \u001b[38;5;241m=\u001b[39m \u001b[43mqmix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# 可视化总回报变化\u001b[39;00m\n\u001b[1;32m    390\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(total_returns)\n",
      "Cell \u001b[0;32mIn[4], line 327\u001b[0m, in \u001b[0;36mQMIX.run\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m#self.adjust_epsilon()\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(episode_data)  \u001b[38;5;66;03m# 在一个episode结束后将整个episode添加到buffer\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 在每个episode结束后进行训练\u001b[39;00m\n\u001b[1;32m    329\u001b[0m total_q_values\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(episode_q_values))\n\u001b[1;32m    330\u001b[0m total_returns\u001b[38;5;241m.\u001b[39mappend(episode_return)\n",
      "Cell \u001b[0;32mIn[4], line 248\u001b[0m, in \u001b[0;36mQMIX.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m {agent: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(all_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrnn_hidden_dim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    246\u001b[0m next_hidden_state \u001b[38;5;241m=\u001b[39m {agent: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(all_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrnn_hidden_dim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m--> 248\u001b[0m q_values \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(states[agent], hidden_state[agent])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions[agent]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    249\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_rnn(next_states[agent], next_hidden_state[agent])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#print(q_values)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m#print(next_q_values)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#print()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 248\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m {agent: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(all_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrnn_hidden_dim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    246\u001b[0m next_hidden_state \u001b[38;5;241m=\u001b[39m {agent: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(all_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrnn_hidden_dim), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m--> 248\u001b[0m q_values \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions[agent]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    249\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_rnn(next_states[agent], next_hidden_state[agent])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#print(q_values)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m#print(next_q_values)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#print()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 129\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, obs, hidden_state)\u001b[0m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(obs))\n\u001b[1;32m    128\u001b[0m h_in \u001b[38;5;241m=\u001b[39m hidden_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrnn_hidden_dim)\n\u001b[0;32m--> 129\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q, h\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:1471\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m     hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1471\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1478\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from env.custom_environment import CustomEnvironment\n",
    "from env.custom_environment_demo import CustomEnvironment_demo\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "\n",
    "# Replay Buffer定义\n",
    "class EpisodeReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        return batch  # 返回完整的batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size, alpha):\n",
    "        self.buffer = []\n",
    "        self.max_size = buffer_size\n",
    "        self.alpha = alpha\n",
    "        self.priorities = []\n",
    "        self.next_idx = 0\n",
    "\n",
    "    def add(self, experience, td_error):\n",
    "        priority = (abs(td_error) + 1e-5) ** self.alpha\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(priority)\n",
    "        else:\n",
    "            self.buffer[self.next_idx] = experience\n",
    "            self.priorities[self.next_idx] = priority\n",
    "        self.next_idx = (self.next_idx + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        if len(self.priorities) == 0:\n",
    "            raise ValueError(\"No priorities available to sample.\")\n",
    "\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = (abs(td_error) + 1e-5) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "\n",
    "# QMIX网络定义\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        if args.two_hyper_layers:\n",
    "            self.hyper_w1 = nn.Sequential(nn.Linear(args.state_shape * args.n_agents, args.hyper_hidden_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(args.hyper_hidden_dim, args.n_agents * args.qmix_hidden_dim))\n",
    "            self.hyper_w2 = nn.Sequential(nn.Linear(args.state_shape * args.n_agents, args.hyper_hidden_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(args.hyper_hidden_dim, args.qmix_hidden_dim * 1))\n",
    "        else:\n",
    "            self.hyper_w1 = nn.Linear(args.state_shape * args.n_agents, args.n_agents * args.qmix_hidden_dim)\n",
    "            self.hyper_w2 = nn.Linear(args.state_shape * args.n_agents, args.qmix_hidden_dim * 1)\n",
    "\n",
    "        self.hyper_b1 = nn.Linear(args.state_shape * args.n_agents, args.qmix_hidden_dim)\n",
    "        self.hyper_b2 = nn.Sequential(nn.Linear(args.state_shape * args.n_agents, args.qmix_hidden_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(args.qmix_hidden_dim, 1))\n",
    "\n",
    "    def forward(self, q_values, states):\n",
    "        episode_num = q_values.size(0)\n",
    "        q_values = q_values.view(episode_num, 1, self.args.n_agents)\n",
    "        states = states.view(episode_num, -1)\n",
    "\n",
    "        w1 = torch.abs(self.hyper_w1(states))\n",
    "        b1 = self.hyper_b1(states)\n",
    "        w1 = w1.view(episode_num, self.args.n_agents, self.args.qmix_hidden_dim)\n",
    "        b1 = b1.view(episode_num, 1, self.args.qmix_hidden_dim)\n",
    "\n",
    "        hidden = F.elu(torch.bmm(q_values, w1) + b1)\n",
    "        w2 = torch.abs(self.hyper_w2(states))\n",
    "        b2 = self.hyper_b2(states)\n",
    "        w2 = w2.view(episode_num, self.args.qmix_hidden_dim, 1)\n",
    "        b2 = b2.view(episode_num, 1, 1)\n",
    "\n",
    "        q_total = torch.bmm(hidden, w2) + b2\n",
    "        q_total = q_total.view(episode_num, -1, 1)\n",
    "        return q_total\n",
    "\n",
    "\n",
    "# RNN定义\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(RNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.rnn_hidden_dim, args.n_actions)\n",
    "\n",
    "    def forward(self, obs, hidden_state):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, h\n",
    "    \n",
    "\n",
    "'''class RNN(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(RNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)\n",
    "        self.attention_fc = nn.Linear(args.rnn_hidden_dim, 1)  # 添加注意力层\n",
    "        self.fc2 = nn.Linear(args.rnn_hidden_dim, args.n_actions)\n",
    "\n",
    "    def forward(self, obs, hidden_state):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        \n",
    "        # 注意力机制\n",
    "        attn_weights = F.softmax(self.attention_fc(h), dim=0)  # 计算注意力权重\n",
    "        h = h * attn_weights  # 加权隐藏状态\n",
    "\n",
    "        q = self.fc2(h)\n",
    "        return q, h'''\n",
    "\n",
    "\n",
    "class QMIX:\n",
    "    def __init__(self, env, args, use_cuda=False):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "        self.replay_buffer = EpisodeReplayBuffer(args.buffer_size)\n",
    "        self.rnn = RNN(args.obs_shape, args).to(self.device)\n",
    "        self.qmix_net = QMixNet(args).to(self.device)\n",
    "\n",
    "        self.target_rnn = RNN(args.obs_shape, args).to(self.device)\n",
    "        self.target_qmix_net = QMixNet(args).to(self.device)\n",
    "\n",
    "        self.optimizer = Adam(list(self.rnn.parameters()) + list(self.qmix_net.parameters()), lr=args.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.update_target_network()\n",
    "        self.total_steps = 0  # 增加一个计数器来记录总的训练步数\n",
    "\n",
    "        self.epsilon = self.args.epsilon_start\n",
    "\n",
    "        self.rnn = RNN(args.obs_shape, args).to(self.device)\n",
    "        self.target_rnn = RNN(args.obs_shape, args).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_rnn.load_state_dict(self.rnn.state_dict())\n",
    "        self.target_qmix_net.load_state_dict(self.qmix_net.state_dict())\n",
    "\n",
    "    \n",
    "    def adjust_epsilon(self):\n",
    "        decay_rate = (self.args.epsilon_start - self.args.epsilon_end) / (self.args.num_episodes / 2)\n",
    "        self.epsilon = max(self.args.epsilon_end, self.epsilon - decay_rate)\n",
    "        \n",
    "\n",
    "    def choose_action(self, obs, hidden_state):\n",
    "        actions = {}\n",
    "        new_hidden_state = {}\n",
    "\n",
    "        for agent in self.env.agents:\n",
    "            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values, new_agent_hidden_state = self.rnn(agent_obs, agent_hidden_state)\n",
    "            \n",
    "            #if random.random() < self.epsilon:\n",
    "            if random.random() < 0.01:\n",
    "                #print(random.random())\n",
    "                action = random.choice(range(self.args.n_actions))  # 以 epsilon 的概率随机选择动作\n",
    "            else:\n",
    "                action = q_values.argmax(dim=-1).item()  # 以 1-epsilon 的概率选择 Q 值最大的动作\n",
    "                \n",
    "            \n",
    "            actions[agent] = action\n",
    "            new_hidden_state[agent] = new_agent_hidden_state.detach().cpu().numpy()\n",
    "        #print(self.epsilon)\n",
    "        #print(actions)\n",
    "\n",
    "        return actions, new_hidden_state\n",
    "    \n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.args.minimal_size:\n",
    "            self.adjust_epsilon()\n",
    "            return\n",
    "\n",
    "        episodes = self.replay_buffer.sample(self.args.batch_size)\n",
    "        #print(episodes)\n",
    "        \n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_rewards = []\n",
    "        all_next_states = []\n",
    "        all_dones = []\n",
    "\n",
    "        for episode in episodes:\n",
    "            states, actions, rewards, next_states, dones = zip(*episode)\n",
    "            all_states.extend(states)\n",
    "            all_actions.extend(actions)\n",
    "            all_rewards.extend(rewards)\n",
    "            all_next_states.extend(next_states)\n",
    "            all_dones.extend(dones)\n",
    "\n",
    "        # 将列表转换为numpy.ndarray，然后再转换为张量\n",
    "        states = {agent: torch.tensor(np.array([s[agent] for s in all_states]), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        actions = {agent: torch.tensor(np.array([a[agent] for a in all_actions]), dtype=torch.int64).to(self.device) for agent in self.env.agents}\n",
    "        rewards = {agent: torch.tensor(np.array([r[agent] for r in all_rewards]), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_states = {agent: torch.tensor(np.array([ns[agent] for ns in all_next_states]), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        dones = {agent: torch.tensor(np.array([d[agent] for d in all_dones]), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "\n",
    "        hidden_state = {agent: torch.zeros((len(all_states), self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_hidden_state = {agent: torch.zeros((len(all_states), self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "\n",
    "        q_values = {agent: self.rnn(states[agent], hidden_state[agent])[0].gather(1, actions[agent].unsqueeze(-1)).squeeze(-1) for agent in self.env.agents}\n",
    "        next_q_values = {agent: self.target_rnn(next_states[agent], next_hidden_state[agent])[0].max(dim=-1)[0] for agent in self.env.agents}\n",
    "        #print(q_values)\n",
    "        #print(next_q_values)\n",
    "        #print()\n",
    "        \n",
    "        targets = {agent: rewards[agent] + self.args.gamma * next_q_values[agent] * (1 - dones[agent]) for agent in self.env.agents}\n",
    "\n",
    "        q_values_tensor = torch.stack(list(q_values.values()), dim=1)\n",
    "        targets_tensor = torch.stack(list(targets.values()), dim=1)\n",
    "        states_tensor = torch.cat([states[agent].view(len(all_states), -1) for agent in self.env.agents], dim=1)\n",
    "\n",
    "        q_total = self.qmix_net(q_values_tensor, states_tensor)\n",
    "        targets_total = self.target_qmix_net(targets_tensor, states_tensor)\n",
    "\n",
    "        # td_error = q_total - targets_total\n",
    "        # loss = (td_error ** 2).mean()\n",
    "\n",
    "        loss = self.loss_fn(q_total, targets_total.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 添加梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(self.rnn.parameters(), 0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(self.qmix_net.parameters(), 0.5)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #日志记录\n",
    "        # if self.total_steps % 5 == 0:\n",
    "        #     print(f\"Step {self.total_steps}, Loss: {loss.item()}, Q_total: {q_total.mean().item()}\")\n",
    "\n",
    "        if self.total_steps % self.args.target_update_interval == 0:\n",
    "            self.update_target_network()\n",
    "        self.total_steps += 1\n",
    "\n",
    "        self.adjust_epsilon()\n",
    "\n",
    "    \n",
    "\n",
    "    def run(self, num_episodes):\n",
    "        total_q_values = []\n",
    "        total_returns = []\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "                for episode in range(int(num_episodes / 10)):\n",
    "                    obs = self.env.reset()\n",
    "                    hidden_state = {agent: np.zeros(self.args.rnn_hidden_dim) for agent in self.env.agents}\n",
    "                    episode_q_values = []\n",
    "                    episode_return = 0\n",
    "                    episode_data = []  # 存储整个episode的数据\n",
    "                    actions_over_time = []\n",
    "\n",
    "                    for t in range(int(self.args.num_iterations)):\n",
    "                        actions, hidden_state = self.choose_action(obs, hidden_state)\n",
    "                        next_obs, rewards, dones, _ = self.env.step(actions)\n",
    "                        episode_data.append((obs, actions, rewards, next_obs, dones))  # 存储每个step的数据\n",
    "                        obs = next_obs\n",
    "\n",
    "                        actions_over_time.append(actions)\n",
    "\n",
    "                        q_values = []\n",
    "                        for agent in self.env.agents:\n",
    "                            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_q_values, _ = self.rnn(agent_obs, agent_hidden_state)\n",
    "                            q_values.append(agent_q_values.max().item())\n",
    "                        episode_q_values.append(np.mean(q_values))\n",
    "\n",
    "                        episode_return += sum(rewards.values())\n",
    "\n",
    "                        if all(dones.values()):\n",
    "                            break\n",
    "                    \n",
    "                    #self.adjust_epsilon()\n",
    "\n",
    "                    self.replay_buffer.add(episode_data)  # 在一个episode结束后将整个episode添加到buffer\n",
    "\n",
    "                    self.train()  # 在每个episode结束后进行训练\n",
    "\n",
    "                    total_q_values.append(np.mean(episode_q_values))\n",
    "                    total_returns.append(episode_return)\n",
    "\n",
    "                    if episode_return > 17000:\n",
    "                        print(f\"Episode {episode + 1} reached a total return of {episode_return}\")\n",
    "                        for t, actions in enumerate(actions_over_time):\n",
    "                            #print(f\"Time step {t}:\")\n",
    "                            actions_list = [actions[agent] for agent in self.env.agents]\n",
    "                            print(actions_list)\n",
    "                        print(\"done\")\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix({'episode': '%d' % (num_episodes / 10 * i + episode + 1), 'returns': '%.3f' % np.mean(total_returns[-10:])})\n",
    "                    pbar.update(1)\n",
    "\n",
    "        return total_q_values, total_returns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "class Args:\n",
    "    def __init__(self, env):\n",
    "        self.state_shape = env.max_obs_size  # 状态维度，根据环境设置\n",
    "        self.obs_shape = env.max_obs_size  # 观测维度，根据环境设置\n",
    "        self.n_agents = 3  # 智能体数量\n",
    "        self.n_actions = 2  # 动作数量\n",
    "        self.qmix_hidden_dim = 32\n",
    "        self.hyper_hidden_dim = 64\n",
    "        self.rnn_hidden_dim = 64\n",
    "        self.two_hyper_layers = True\n",
    "        self.buffer_size = 1000000000\n",
    "        self.minimal_size = 100\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 1e-4\n",
    "        self.num_iterations = env.num_iterations  # 每个episode的迭代次数\n",
    "        self.num_episodes = 300 # 总episode数\n",
    "        self.target_update_interval = 20  # 目标网络更新间隔\n",
    "        self.alpha = 0.6  # PER中的α参数\n",
    "        self.beta = 0.4   # PER中的β参数\n",
    "\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "env = CustomEnvironment_demo()\n",
    "seed = 13473 #4538724\n",
    "args = Args(env)\n",
    "# 设置随机种子以确保可复现性\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed * 3)\n",
    "random.seed(seed * 7)\n",
    "\n",
    "# 选择是否使用CUDA\n",
    "use_cuda = False\n",
    "qmix = QMIX(env, args, use_cuda)\n",
    "total_q_values, total_returns = qmix.run(args.num_episodes)\n",
    "\n",
    "# 可视化总回报变化\n",
    "plt.plot(total_returns)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Return')\n",
    "plt.title('Total Return over episodes')\n",
    "plt.show()\n",
    "\n",
    "# 可视化Q值变化\n",
    "plt.plot(total_q_values)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Q value')\n",
    "plt.title('Total Q value over episodes')\n",
    "plt.show()\n",
    "\n",
    "#actions, next_obs, rewards, dones = qmix.run_one_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
