{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:  33%|███▎      | 33/100 [00:27<00:54,  1.22it/s, episode=30, returns=11029.334]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 216\u001b[0m\n\u001b[1;32m    214\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    215\u001b[0m vdn \u001b[38;5;241m=\u001b[39m VDN(env, args, use_cuda)\n\u001b[0;32m--> 216\u001b[0m total_q_values, total_returns \u001b[38;5;241m=\u001b[39m \u001b[43mvdn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# 可视化总回报变化\u001b[39;00m\n\u001b[1;32m    219\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(total_returns)\n",
      "Cell \u001b[0;32mIn[2], line 148\u001b[0m, in \u001b[0;36mVDN.run\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    144\u001b[0m episode_q_values\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(q_values))\n\u001b[1;32m    146\u001b[0m episode_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rewards\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dones\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m, in \u001b[0;36mVDN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(q_total, targets_total\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from env.custom_environment import CustomEnvironment\n",
    "\n",
    "# Replay Buffer定义\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# RNN定义\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(RNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.rnn_hidden_dim, args.n_actions)\n",
    "\n",
    "    def forward(self, obs, hidden_state):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, h\n",
    "\n",
    "\n",
    "# VDNNet定义\n",
    "class VDNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VDNNet, self).__init__()\n",
    "\n",
    "    def forward(self, q_values):\n",
    "        return torch.sum(q_values, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "class VDN:\n",
    "    def __init__(self, env, args, use_cuda=False):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "        self.replay_buffer = ReplayBuffer(args.buffer_size)\n",
    "        self.rnn = RNN(args.obs_shape, args).to(self.device)\n",
    "        self.vdn_net = VDNNet().to(self.device)\n",
    "        self.optimizer = Adam(list(self.rnn.parameters()) + list(self.vdn_net.parameters()), lr=args.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, obs, hidden_state, epsilon=0.01):\n",
    "        actions = {}\n",
    "        new_hidden_state = {}\n",
    "\n",
    "        for agent in self.env.agents:\n",
    "            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values, new_agent_hidden_state = self.rnn(agent_obs, agent_hidden_state)\n",
    "            \n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(range(self.args.n_actions))  # 以 ε 的概率随机选择动作\n",
    "            else:\n",
    "                action = q_values.argmax(dim=-1).item()  # 以 1-ε 的概率选择 Q 值最大的动作\n",
    "                \n",
    "            actions[agent] = action\n",
    "            new_hidden_state[agent] = new_agent_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "        return actions, new_hidden_state\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.args.minimal_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.args.batch_size)\n",
    "        states = {agent: torch.tensor([s[agent] for s in states], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        actions = {agent: torch.tensor([a[agent] for a in actions], dtype=torch.int64).to(self.device) for agent in self.env.agents}\n",
    "        rewards = {agent: torch.tensor([r[agent] for r in rewards], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_states = {agent: torch.tensor([ns[agent] for ns in next_states], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        dones = {agent: torch.tensor([d[agent] for d in dones], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "\n",
    "        hidden_state = {agent: torch.zeros((self.args.batch_size, self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_hidden_state = {agent: torch.zeros((self.args.batch_size, self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "\n",
    "        q_values = {agent: self.rnn(states[agent], hidden_state[agent])[0].gather(1, actions[agent].unsqueeze(-1)).squeeze(-1) for agent in self.env.agents}\n",
    "        next_q_values = {agent: self.rnn(next_states[agent], next_hidden_state[agent])[0].max(dim=-1)[0] for agent in self.env.agents}\n",
    "\n",
    "        targets = {agent: rewards[agent] + self.args.gamma * next_q_values[agent] * (1 - dones[agent]) for agent in self.env.agents}\n",
    "\n",
    "        q_values_tensor = torch.stack(list(q_values.values()), dim=1)\n",
    "        targets_tensor = torch.stack(list(targets.values()), dim=1)\n",
    "\n",
    "        q_total = self.vdn_net(q_values_tensor)\n",
    "        targets_total = self.vdn_net(targets_tensor)\n",
    "\n",
    "        loss = self.loss_fn(q_total, targets_total.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self, num_episodes):\n",
    "        total_q_values = []\n",
    "        total_returns = []\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "                for episode in range(int(num_episodes / 10)):\n",
    "                    obs = self.env.reset()  # 重置环境，获取初始观测\n",
    "                    hidden_state = {agent: np.zeros(self.args.rnn_hidden_dim) for agent in self.env.agents}  # 初始化每个智能体的隐藏状态\n",
    "                    episode_q_values = []\n",
    "                    episode_return = 0\n",
    "                    actions_over_time = []\n",
    "\n",
    "                    for t in range(int(self.args.num_iterations)):\n",
    "                        actions, hidden_state = self.choose_action(obs, hidden_state)\n",
    "                        next_obs, rewards, dones, _ = self.env.step(actions)\n",
    "                        self.replay_buffer.add((obs, actions, rewards, next_obs, dones))\n",
    "                        obs = next_obs\n",
    "\n",
    "                        actions_over_time.append(actions)\n",
    "\n",
    "                        q_values = []\n",
    "                        for agent in self.env.agents:\n",
    "                            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_q_values, _ = self.rnn(agent_obs, agent_hidden_state)\n",
    "                            q_values.append(agent_q_values.max().item())\n",
    "                        episode_q_values.append(np.mean(q_values))\n",
    "\n",
    "                        episode_return += sum(rewards.values())\n",
    "\n",
    "                        self.train()\n",
    "\n",
    "                        if all(dones.values()):\n",
    "                            break\n",
    "\n",
    "                    total_q_values.append(np.mean(episode_q_values))\n",
    "                    total_returns.append(episode_return)\n",
    "\n",
    "                    #print(episode_return)\n",
    "                    if episode_return > 17000:\n",
    "                        print(f\"Episode {episode + 1} reached a total return of {episode_return}\")\n",
    "                        for t, actions in enumerate(actions_over_time):\n",
    "                            actions_list = [actions[agent] for agent in self.env.agents]\n",
    "                            print(actions_list)\n",
    "                        print(\"done\")\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix({'episode': '%d' % (num_episodes / 10 * i + episode + 1), 'returns': '%.3f' % np.mean(total_returns[-10:])})\n",
    "                    pbar.update(1)\n",
    "\n",
    "        return total_q_values, total_returns\n",
    "\n",
    "    def run_one_iteration(self):\n",
    "        obs = self.env.reset()  # 重置环境，获取初始观测\n",
    "        hidden_state = {agent: np.zeros(self.args.rnn_hidden_dim) for agent in self.env.agents}  # 初始化每个智能体的隐藏状态\n",
    "\n",
    "        actions_over_time = []\n",
    "\n",
    "        for t in range(self.args.num_iterations):\n",
    "            actions, hidden_state = self.choose_action(obs, hidden_state)\n",
    "            next_obs, rewards, dones, _ = self.env.step(actions)\n",
    "\n",
    "            actions_over_time.append(actions)\n",
    "\n",
    "            if all(dones.values()):\n",
    "                break\n",
    "            obs = next_obs\n",
    "\n",
    "        for t, actions in enumerate(actions_over_time):\n",
    "            print(f\"Time step {t}:\")\n",
    "            for agent, action in actions.items():\n",
    "                print(f\"  Agent {agent}: Action {action}\")\n",
    "\n",
    "        return actions, next_obs, rewards, dones\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "class Args:\n",
    "    def __init__(self, env):\n",
    "        self.state_shape = env.max_obs_size  # 状态维度，根据环境设置\n",
    "        self.obs_shape = env.max_obs_size  # 观测维度，根据环境设置\n",
    "        self.n_agents = 5  # 智能体数量\n",
    "        self.n_actions = 2  # 动作数量\n",
    "        self.rnn_hidden_dim = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.minimal_size = 100\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.95\n",
    "        self.lr = 1e-3\n",
    "        self.num_iterations = env.num_iterations  # 每个episode的迭代次数\n",
    "        self.num_episodes = 1000  # 总episode数\n",
    "\n",
    "env = CustomEnvironment()\n",
    "torch.manual_seed(0)\n",
    "args = Args(env)\n",
    "\n",
    "# 选择是否使用CUDA\n",
    "use_cuda = False\n",
    "vdn = VDN(env, args, use_cuda)\n",
    "total_q_values, total_returns = vdn.run(args.num_episodes)\n",
    "\n",
    "# 可视化总回报变化\n",
    "plt.plot(total_returns)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Return')\n",
    "plt.title('Total Return over episodes')\n",
    "plt.show()\n",
    "\n",
    "# 可视化Q值变化\n",
    "plt.plot(total_q_values)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Q value')\n",
    "plt.title('Total Q value over episodes')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
