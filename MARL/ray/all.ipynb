{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境注册成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 21:27:45,898\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-11-17 21:27:46,546\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-17 21:27:55</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:08.83        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.0/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_bbb69_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-11-17_21-27-44_258649_70807/artifacts/2024-11-17_21-27-46/PPO_2024-11-17_21-27-46/driver_artifacts/PPO_env_bbb69_00000_0_2024-11-17_21-27-46/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_bbb69_00000</td><td>ERROR   </td><td>127.0.0.1:71216</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.503247</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">   26943</td><td style=\"text-align: right;\">               26943</td><td style=\"text-align: right;\">               26943</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:282 -- observation_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:246 -- action_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:209 -- action_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=71228)\u001b[0m 2024-11-17 21:27:53,756\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(PPO pid=71216)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m 2024-11-17 21:27:54,915\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                      </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                               </th><th>policy_reward_max                   </th><th>policy_reward_mean                  </th><th>policy_reward_min                   </th><th>sampler_perf                                                                                                                                                                                                      </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th>timers                                                                                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_bbb69_00000</td><td style=\"text-align: right;\">                   1000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.004267692565917969, &#x27;StateBufferConnector_ms&#x27;: 0.0036954879760742188, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.121307373046875}</td><td>{&#x27;num_env_steps_sampled&#x27;: 200, &#x27;num_env_steps_trained&#x27;: 200, &#x27;num_agent_steps_sampled&#x27;: 1000, &#x27;num_agent_steps_trained&#x27;: 1000}</td><td>{}              </td><td style=\"text-align: right;\">               200</td><td>{}             </td><td style=\"text-align: right;\">               26943</td><td style=\"text-align: right;\">                26943</td><td style=\"text-align: right;\">               26943</td><td style=\"text-align: right;\">                   1</td><td>{&#x27;learner&#x27;: {&#x27;shared_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 0.21767616760917008, &#x27;cur_kl_coeff&#x27;: 0.19999999999999998, &#x27;cur_lr&#x27;: 0.0003, &#x27;total_loss&#x27;: 9.99299510717392, &#x27;policy_loss&#x27;: -0.010902766877552494, &#x27;vf_loss&#x27;: 10.0, &#x27;vf_explained_var&#x27;: 2.995133399963379e-07, &#x27;kl&#x27;: 0.01948941615937656, &#x27;entropy&#x27;: 0.6736863292753696, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 62.5, &#x27;num_grad_updates_lifetime&#x27;: 80.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 79.5}}, &#x27;num_env_steps_sampled&#x27;: 200, &#x27;num_env_steps_trained&#x27;: 200, &#x27;num_agent_steps_sampled&#x27;: 1000, &#x27;num_agent_steps_trained&#x27;: 1000}</td><td style=\"text-align: right;\">                     1000</td><td style=\"text-align: right;\">                     1000</td><td style=\"text-align: right;\">                    200</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                                   399.517</td><td style=\"text-align: right;\">                    200</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                                   399.517</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 0.0, &#x27;ram_util_percent&#x27;: 74.7}</td><td>{&#x27;shared_policy&#x27;: 5528.591453473173}</td><td>{&#x27;shared_policy&#x27;: 5388.591453473173}</td><td>{&#x27;shared_policy&#x27;: 5253.591453473173}</td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.3542117218473064, &#x27;mean_inference_ms&#x27;: 0.3190372713762729, &#x27;mean_action_processing_ms&#x27;: 0.12780303385720324, &#x27;mean_env_wait_ms&#x27;: 0.036243182509692745, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 26942.957267365862, &#x27;episode_reward_min&#x27;: 26942.957267365862, &#x27;episode_reward_mean&#x27;: 26942.957267365862, &#x27;episode_len_mean&#x27;: 200.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 1, &#x27;policy_reward_min&#x27;: {&#x27;shared_policy&#x27;: 5253.591453473173}, &#x27;policy_reward_max&#x27;: {&#x27;shared_policy&#x27;: 5528.591453473173}, &#x27;policy_reward_mean&#x27;: {&#x27;shared_policy&#x27;: 5388.591453473173}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [26942.957267365862], &#x27;episode_lengths&#x27;: [200], &#x27;policy_shared_policy_reward&#x27;: [5528.591453473173, 5278.591453473173, 5253.591453473173, 5503.591453473173, 5378.591453473173]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.3542117218473064, &#x27;mean_inference_ms&#x27;: 0.3190372713762729, &#x27;mean_action_processing_ms&#x27;: 0.12780303385720324, &#x27;mean_env_wait_ms&#x27;: 0.036243182509692745, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.004267692565917969, &#x27;StateBufferConnector_ms&#x27;: 0.0036954879760742188, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.121307373046875}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 500.555, &#x27;sample_time_ms&#x27;: 173.136, &#x27;learn_time_ms&#x27;: 325.906, &#x27;learn_throughput&#x27;: 613.674, &#x27;synch_weights_time_ms&#x27;: 0.967}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=71216)\u001b[0m 2024-11-17 21:27:55,242\tWARNING ppo.py:620 -- The mean reward returned from the environment is 26.942956924438477 but the vf_clip_param is set to 10.0. Consider increasing it for policy: shared_policy to improve value function convergence.\n",
      "2024-11-17 21:27:55,373\tERROR tune_controller.py:1332 -- Trial task failed for trial PPO_env_bbb69_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::PPO.train()\u001b[39m (pid=71216, ip=127.0.0.1, actor_id=40ca56a774268afa69c8cf2901000000, repr=PPO)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 334, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 849, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 3194, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 410, in training_step\n",
      "    return self._training_step_old_and_hybrid_api_stacks()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 495, in _training_step_old_and_hybrid_api_stacks\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 88, in synchronous_parallel_sample\n",
      "    sampled_data = worker_set.foreach_worker(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 771, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 78, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=71228, ip=127.0.0.1, actor_id=6c02c2703935817c555d38d101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1690b2e20>)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    raise e\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 89, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 694, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 273, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in run\n",
      "    outputs = self.step()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 374, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 641, in _process_observations\n",
      "    processed = policy.agent_connectors(acd_list)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/agent/pipeline.py\", line 41, in __call__\n",
      "    ret = c(ret)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/connector.py\", line 265, in __call__\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/connector.py\", line 265, in <listcomp>\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/agent/obs_preproc.py\", line 55, in transform\n",
      "    d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 213, in transform\n",
      "    self.check_shape(observation)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 71, in check_shape\n",
      "    raise ValueError(\n",
      "ValueError: Observation ([-0.6680629  -1.0054743  -0.6508983  -0.88644314 -1.104041  ] dtype=float32) outside given space (Box(-1.0, 1.0, (5,), float32))!\n",
      "2024-11-17 21:27:55,384\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/Users/cyj/ray_results/PPO_2024-11-17_21-27-46' in 0.0061s.\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m 2024-11-17 21:27:55,368\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=71228, ip=127.0.0.1, actor_id=6c02c2703935817c555d38d101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1690b2e20>)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     raise e\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 89, in <lambda>\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 694, in sample\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 273, in get_data\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in run\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     outputs = self.step()\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 374, in step\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 641, in _process_observations\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/agent/pipeline.py\", line 41, in __call__\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     ret = c(ret)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/connector.py\", line 265, in __call__\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/connector.py\", line 265, in <listcomp>\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/connectors/agent/obs_preproc.py\", line 55, in transform\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 213, in transform\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 71, in check_shape\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(PPO pid=71216)\u001b[0m ValueError: Observation ([-0.6680629  -1.0054743  -0.6508983  -0.88644314 -1.104041  ] dtype=float32) outside given space (Box(-1.0, 1.0, (5,), float32))!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练出错: ('Trials did not complete', [PPO_env_bbb69_00000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "\n",
    "# 定义环境类\n",
    "class MAEnvironment(MultiAgentEnv):\n",
    "    def __init__(self, num_agents=5, num_iterations=200, dt=0.1):\n",
    "        super().__init__()  # 调用父类初始化\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [\"agent_\" + str(i) for i in range(num_agents)]\n",
    "        self.agent_name_mapping = dict(zip(self.agents, list(range(num_agents))))\n",
    "        self._agent_ids = set(self.agents)  # 添加 _agent_ids 属性\n",
    "\n",
    "        # 初始化其他属性\n",
    "        self.num_iterations = num_iterations\n",
    "        self.dt = dt\n",
    "        self.current_iteration = 0\n",
    "\n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "\n",
    "        self.epsilon = 0.005\n",
    "        self.time_to_reach_epsilon = None\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.max_obs_size = self.compute_max_obs_size()\n",
    "    \n",
    "    def compute_max_obs_size(self):\n",
    "        max_neighbors = max(len(agent.neighbors) for agent in self.agent_objs)\n",
    "        return 1 + max_neighbors\n",
    "    \n",
    "    def init_neighbors(self):\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[1])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[3].add_neighbor(self.agent_objs[4])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "        self.current_iteration = 0\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.time_to_reach_epsilon = None\n",
    "        \n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}  # 返回额外的 per-agent 信息字典\n",
    "        return observations, infos\n",
    "\n",
    "    def get_observation(self, agent):\n",
    "        agent_index = self.agent_name_mapping[agent]\n",
    "        agent_obj = self.agent_objs[agent_index]\n",
    "        neighbors_positions = [neighbor.position for neighbor in agent_obj.neighbors]\n",
    "        obs = np.array([agent_obj.position] + neighbors_positions, dtype=np.float32)\n",
    "        \n",
    "        # 填充观测到最大观测大小\n",
    "        if len(obs) < self.max_obs_size:\n",
    "            padding = np.zeros(self.max_obs_size - len(obs))\n",
    "            obs = np.concatenate([obs, padding])\n",
    "        \n",
    "        # 不进行裁剪\n",
    "        return obs\n",
    "\n",
    "    def compute_average_position_difference(self):\n",
    "        total_difference = 0\n",
    "        count = 0\n",
    "        for i, agent_i in enumerate(self.agent_objs):\n",
    "            for j, agent_j in enumerate(self.agent_objs):\n",
    "                if i < j:\n",
    "                    total_difference += abs(agent_i.position - agent_j.position)\n",
    "                    count += 1\n",
    "        if count > 0:\n",
    "            return total_difference / count\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        triggers = np.array([action_dict.get(agent, 0) for agent in self.agents])  # 确保访问安全\n",
    "        trigger_count = np.sum(triggers)\n",
    "        self.total_trigger_count += trigger_count\n",
    "\n",
    "        for i, agent in enumerate(self.agent_objs):\n",
    "            agent.update_position(self.current_iteration, self.dt, triggers[i])\n",
    "\n",
    "        self.all_within_epsilon = all(all(abs(agent.position - neighbor.position) < self.epsilon for neighbor in agent.neighbors) for agent in self.agent_objs)\n",
    "\n",
    "        if self.all_within_epsilon:\n",
    "            if self.epsilon_violated:\n",
    "                self.time_to_reach_epsilon = self.current_iteration\n",
    "                self.epsilon_violated = False\n",
    "                self.time_to_reach_epsilon_changes += 1\n",
    "        else:\n",
    "            self.epsilon_violated = True\n",
    "            self.time_to_reach_epsilon = None\n",
    "        \n",
    "        self.current_iteration += 1\n",
    "        terminated = self.current_iteration >= self.num_iterations\n",
    "\n",
    "        rewards = {}\n",
    "        if not terminated:\n",
    "            average_position_difference = self.compute_average_position_difference()\n",
    "            for agent in self.agents:\n",
    "                if self.time_to_reach_epsilon is not None:\n",
    "                    rewards[agent] = 20 if action_dict.get(agent, 0) == 0 else -5  # 动作为0奖励，1惩罚\n",
    "                else:\n",
    "                    rewards[agent] = -20 - 5 * np.abs(average_position_difference)\n",
    "        else:\n",
    "            if self.time_to_reach_epsilon is not None:\n",
    "                global_reward = 5000 - self.time_to_reach_epsilon - self.total_trigger_count\n",
    "            else:\n",
    "                global_reward = -5000\n",
    "            for agent in self.agents:\n",
    "                rewards[agent] = global_reward\n",
    "\n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        terminateds = {agent: terminated for agent in self.agents}\n",
    "        terminateds[\"__all__\"] = terminated\n",
    "        truncateds = {agent: False for agent in self.agents}  # 无需提前结束\n",
    "        truncateds[\"__all__\"] = False\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        positions = [agent.position for agent in self.agent_objs]\n",
    "        print(f\"Positions: {positions}\")\n",
    "    \n",
    "    def observation_space(self, agent):\n",
    "        return spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_obs_size,), dtype=np.float32)\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "    class Agent:\n",
    "        def __init__(self, initial_position, index):\n",
    "            self.position = initial_position\n",
    "            self.index = index\n",
    "            self.neighbors = []\n",
    "            self.last_broadcast_position = self.position\n",
    "            self.trigger_points = []\n",
    "            self.u_i = 0\n",
    "\n",
    "        def add_neighbor(self, neighbor):\n",
    "            if neighbor not in self.neighbors:\n",
    "                self.neighbors.append(neighbor)\n",
    "                neighbor.neighbors.append(self)\n",
    "\n",
    "        def update_position(self, t, dt, trigger):\n",
    "            if trigger == 1 or t == 0:\n",
    "                self.u_i = -sum((self.last_broadcast_position - neighbor.last_broadcast_position) for neighbor in self.neighbors)\n",
    "                self.position += self.u_i * dt\n",
    "                self.last_broadcast_position = self.position\n",
    "                self.trigger_points.append((t, self.position))\n",
    "            else:\n",
    "                self.position += self.u_i * dt\n",
    "\n",
    "# 环境创建函数\n",
    "def env_creator(config):\n",
    "    return MAEnvironment(num_agents=config.get(\"num_agents\", 5))\n",
    "\n",
    "# 注册环境\n",
    "register_env(\"env\", lambda config: MAEnvironment(num_agents=config.get(\"num_agents\", 5)))\n",
    "print(\"环境注册成功\")\n",
    "\n",
    "\n",
    "# 定义共享策略的映射函数\n",
    "def shared_policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    return \"shared_policy\"\n",
    "\n",
    "# 启动 Ray\n",
    "ray.shutdown() \n",
    "ray.init()\n",
    "\n",
    "# 配置\n",
    "config = {\n",
    "    \"env\": \"env\",  # 使用注册的环境名\n",
    "    \"env_config\": {\n",
    "        \"num_agents\": 5,  # 传递环境的配置参数\n",
    "    },\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"shared_policy\": (None,  # 使用默认模型\n",
    "                              env_creator({\"num_agents\": 5}).observation_space(\"agent_0\"),  # 观测空间\n",
    "                              env_creator({\"num_agents\": 5}).action_space(\"agent_0\"),  # 动作空间\n",
    "                              {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": shared_policy_mapping_fn,  # 使用共享策略映射\n",
    "    },\n",
    "    \"framework\": \"torch\",  # 使用 \"torch\" 或 \"tf\"\n",
    "    \"num_workers\": 1,  # 使用的工作线程数\n",
    "    \"train_batch_size\": 200,\n",
    "    \"sgd_minibatch_size\": 64,\n",
    "    \"lr\": 0.0003,\n",
    "    \"num_sgd_iter\": 10,\n",
    "}\n",
    "# print(\"配置成功\")\n",
    "# env = env_creator({\"num_agents\": 5})\n",
    "# print(f\"环境创建成功: {env}\")\n",
    "# print(_global_registry.contains(ENV_CREATOR, \"env\"))\n",
    "# 使用 PPO 训练器进行训练\n",
    "\n",
    "print(\"开始训练\")\n",
    "try:\n",
    "    tune.run(PPO, config=config, stop={\"training_iteration\": 200})\n",
    "except Exception as e:\n",
    "    print(f\"训练出错: {e}\")\n",
    "#tune.run(PPO, config=config, stop={\"training_iteration\": 200})\n",
    "\n",
    "# 关闭 Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
