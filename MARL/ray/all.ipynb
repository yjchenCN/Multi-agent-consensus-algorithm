{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境注册成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 20:29:59,184\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-11-23 20:29:59,744\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-23 20:34:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:02.01        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.2/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_a7d11_00000</td><td>TERMINATED</td><td>127.0.0.1:80966</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         231.699</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-6199.74</td><td style=\"text-align: right;\">              4957.1</td><td style=\"text-align: right;\">            -51819.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:282 -- observation_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:246 -- action_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:209 -- action_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=80975)\u001b[0m 2024-11-23 20:30:07,856\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m 2024-11-23 20:30:09,626\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                               </th><th>counters                                                                                                                                  </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th>policy_reward_max                    </th><th>policy_reward_mean                    </th><th>policy_reward_min                     </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>timers                                                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_a7d11_00000</td><td style=\"text-align: right;\">                1000000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0045549869537353516, &#x27;StateBufferConnector_ms&#x27;: 0.011849403381347656, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.13973164558410645}</td><td>{&#x27;num_env_steps_sampled&#x27;: 200000, &#x27;num_env_steps_trained&#x27;: 200000, &#x27;num_agent_steps_sampled&#x27;: 1000000, &#x27;num_agent_steps_trained&#x27;: 1000000}</td><td>{}              </td><td style=\"text-align: right;\">               200</td><td>{}             </td><td style=\"text-align: right;\">              4957.1</td><td style=\"text-align: right;\">             -6199.74</td><td style=\"text-align: right;\">            -51819.2</td><td style=\"text-align: right;\">                  20</td><td>{&#x27;learner&#x27;: {&#x27;shared_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 0.37853255993391893, &#x27;cur_kl_coeff&#x27;: 0.020022583007812504, &#x27;cur_lr&#x27;: 0.0003, &#x27;total_loss&#x27;: 9.93772621939454, &#x27;policy_loss&#x27;: -0.0005587956591809899, &#x27;vf_loss&#x27;: 9.93808272941203, &#x27;vf_explained_var&#x27;: -5.590097059177447e-05, &#x27;kl&#x27;: 0.0101021480373283, &#x27;entropy&#x27;: 0.6696904824881614, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 253.16455696202533, &#x27;num_grad_updates_lifetime&#x27;: 78210.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 789.5}}, &#x27;num_env_steps_sampled&#x27;: 200000, &#x27;num_env_steps_trained&#x27;: 200000, &#x27;num_agent_steps_sampled&#x27;: 1000000, &#x27;num_agent_steps_trained&#x27;: 1000000}</td><td style=\"text-align: right;\">                  1000000</td><td style=\"text-align: right;\">                  1000000</td><td style=\"text-align: right;\">                 200000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   906.146</td><td style=\"text-align: right;\">                 200000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   906.146</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    4</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 19.066666666666666, &#x27;ram_util_percent&#x27;: 76.01666666666667}</td><td>{&#x27;shared_policy&#x27;: 1018.7193657334625}</td><td>{&#x27;shared_policy&#x27;: -1239.9480124159136}</td><td>{&#x27;shared_policy&#x27;: -10363.836856682494}</td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 1.9757700993768967, &#x27;mean_inference_ms&#x27;: 0.5803132026572598, &#x27;mean_action_processing_ms&#x27;: 0.7312221542513175, &#x27;mean_env_wait_ms&#x27;: 0.1695161718845582, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 4957.096828667312, &#x27;episode_reward_min&#x27;: -51819.18428341247, &#x27;episode_reward_mean&#x27;: -6199.74006207957, &#x27;episode_len_mean&#x27;: 200.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 20, &#x27;policy_reward_min&#x27;: {&#x27;shared_policy&#x27;: -10363.836856682494}, &#x27;policy_reward_max&#x27;: {&#x27;shared_policy&#x27;: 1018.7193657334625}, &#x27;policy_reward_mean&#x27;: {&#x27;shared_policy&#x27;: -1239.9480124159136}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [4542.384730979764, -50439.18787125811, 3006.0930279059994, 3922.0722114895016, 2799.2643644514937, 4513.89712403498, 4046.0790670295028, 4447.564985040659, 3069.252700076649, 4475.984653251458, 4006.8228630009044, 3479.661533934509, 4485.277374804949, 3975.2913283970743, 3586.9332672620653, 3167.5616902727506, -51819.18428341247, 4304.489470304824, 3451.3393427318533, 4005.3280224775085, 4003.612050559751, 4179.460897047105, 3426.658112255497, 4012.747409247977, 3400.259361295107, 4544.50747937107, 3804.9870031698947, 3265.85341444167, 4233.486777135119, 4119.312451960033, 3798.549592980332, 4053.7106992241497, 3529.545654991619, 4280.082465722215, 4462.298232273359, 3980.0882983700008, 4455.901386576779, 4007.3626803588236, 2703.185107078576, 4078.344924551222, 4414.5648226340545, 4527.012463324431, 3484.144063174046, 3510.5120963580925, 4618.924509978505, 4281.502979562494, 4240.003308719048, 4322.020537149992, 4441.977474088517, 4381.095902587298, 3742.8398937726197, 4320.674124542302, 3938.8588510256777, 4148.203005694026, 4218.39235284581, 4291.833743431275, 4224.4169814290635, 4752.242500362259, 3022.2390563001836, 4413.638702747708, -51455.11323307004, -51667.78758675697, -51816.79101732272, 2479.687922298451, -51733.96258438194, -51681.02438534392, 1740.340404866709, -51392.05798344302, 2638.2059547950817, 2291.4314432424944, 2861.1409294286364, -51595.117115313595, 3011.99204097157, 3010.77015577146, -51259.3855703647, -51690.32660248343, 3485.042254504341, -51643.91175403286, -51774.95366644313, -51697.52607665984, 2549.335437469599, 2389.86106855143, 3395.841538438559, 2473.831164220789, -51621.08328480527, 3475.496859213866, 2777.85924559523, 2863.880937541474, -51575.749510941736, 3574.2948113050365, 2976.627263073421, 4034.2680752616607, 2861.344454145565, -50918.12300648797, 2965.8121499101967, 4957.096828667312, 4754.812390089368, -50570.62959962831, 3709.813722770126, 4178.772746276622], &#x27;episode_lengths&#x27;: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], &#x27;policy_shared_policy_reward&#x27;: [927.176946195953, 888.676946195953, 939.676946195953, 908.676946195953, 878.176946195953, -10101.437574251622, -10078.437574251622, -10099.437574251622, -10087.437574251622, -10072.437574251622, 591.1186055811996, 579.6186055811996, 605.1186055811996, 604.1186055811996, 626.1186055811996, 765.9144422979003, 774.4144422979003, 798.4144422979003, 796.4144422979003, 786.9144422979003, 598.3528728902986, 542.3528728902986, 545.8528728902985, 563.3528728902986, 549.3528728902986, 896.979424806996, 920.479424806996, 889.979424806996, 921.479424806996, 884.979424806996, 803.0158134059004, 811.0158134059004, 793.5158134059004, 828.5158134059004, 810.0158134059004, 894.1129970081316, 884.1129970081316, 909.6129970081316, 892.1129970081316, 867.6129970081316, 625.0505400153297, 593.5505400153297, 593.5505400153297, 653.0505400153297, 604.0505400153297, 872.6969306502916, 908.6969306502916, 870.1969306502916, 905.1969306502916, 919.1969306502916, 802.464572600181, 812.964572600181, 781.464572600181, 811.964572600181, 797.964572600181, 696.6323067869015, 696.6323067869015, 700.1323067869015, 682.6323067869015, 703.6323067869015, 927.15547496099, 867.15547496099, 898.15547496099, 913.15547496099, 879.65547496099, 799.9582656794147, 747.4582656794147, 817.4582656794147, 782.4582656794147, 827.9582656794147, 695.1866534524129, 714.6866534524129, 728.1866534524129, 719.6866534524129, 729.1866534524129, 618.1123380545503, 621.6123380545503, 614.6123380545503, 674.1123380545503, 639.1123380545503, -10363.836856682494, -10363.836856682494, -10363.836856682494, -10363.836856682494, -10363.836856682494, 845.4978940609649, 862.9978940609649, 887.4978940609649, 852.4978940609649, 855.9978940609649, 674.0678685463706, 676.5678685463707, 705.5678685463706, 707.0678685463707, 688.0678685463706, 811.5656044955017, 801.0656044955018, 773.0656044955018, 829.0656044955017, 790.5656044955017, 781.32241011195, 808.32241011195, 837.32241011195, 792.82241011195, 783.82241011195, 810.4921794094212, 830.4921794094212, 881.4921794094212, 821.9921794094212, 834.9921794094212, 727.7316224510995, 705.7316224510995, 657.7316224510995, 660.2316224510995, 675.2316224510995, 803.2494818495956, 794.7494818495956, 800.7494818495956, 828.7494818495956, 785.2494818495956, 718.5518722590216, 687.0518722590216, 676.5518722590216, 680.0518722590216, 638.0518722590216, 939.2014958742141, 886.7014958742141, 913.7014958742141, 892.2014958742141, 912.7014958742141, 780.4974006339789, 736.4974006339789, 768.9974006339789, 770.4974006339789, 748.4974006339789, 658.770682888334, 634.270682888334, 651.770682888334, 669.270682888334, 651.770682888334, 822.6973554270239, 838.6973554270239, 809.6973554270239, 897.6973554270239, 864.6973554270239, 798.4624903920068, 834.4624903920068, 833.4624903920068, 826.4624903920068, 826.4624903920068, 734.0099185960664, 802.0099185960664, 752.0099185960664, 771.5099185960664, 739.0099185960664, 838.7421398448299, 848.7421398448299, 795.7421398448299, 764.7421398448299, 805.7421398448299, 711.5091309983238, 708.0091309983238, 701.0091309983238, 708.0091309983238, 701.0091309983238, 859.4164931444429, 853.9164931444429, 824.4164931444429, 869.9164931444429, 872.4164931444429, 881.9596464546717, 921.9596464546717, 893.9596464546717, 857.9596464546717, 906.4596464546717, 786.6176596740002, 821.6176596740002, 798.1176596740002, 770.6176596740002, 803.1176596740002, 867.8802773153559, 908.8802773153559, 915.3802773153559, 911.3802773153559, 852.3802773153559, 806.4725360717647, 853.4725360717647, 756.9725360717647, 787.9725360717647, 802.4725360717647, 538.5370214157151, 542.0370214157151, 521.0370214157151, 542.0370214157151, 559.5370214157151, 781.8689849102444, 835.3689849102444, 843.3689849102444, 829.8689849102444, 787.8689849102444, 860.312964526811, 886.312964526811, 873.312964526811, 884.812964526811, 909.812964526811, 894.1024926648863, 905.6024926648863, 885.1024926648863, 908.1024926648863, 934.1024926648863, 689.0288126348094, 699.5288126348094, 714.5288126348094, 690.0288126348094, 691.0288126348094, 693.0024192716187, 696.5024192716187, 717.5024192716187, 689.5024192716186, 714.0024192716186, 924.2849019957009, 927.7849019957009, 916.2849019957009, 912.7849019957009, 937.7849019957009, 846.6005959124989, 848.6005959124989, 864.1005959124989, 895.6005959124989, 826.6005959124989, 848.7006617438096, 843.2006617438096, 808.2006617438096, 880.2006617438096, 859.7006617438096, 837.3041074299985, 858.3041074299985, 879.3041074299985, 871.3041074299985, 875.8041074299985, 899.1954948177034, 884.1954948177034, 888.6954948177034, 880.6954948177034, 889.1954948177033, 891.5191805174595, 850.5191805174595, 918.5191805174595, 876.5191805174595, 844.0191805174595, 739.467978754524, 735.967978754524, 749.967978754524, 763.967978754524, 753.467978754524, 875.2348249084605, 841.2348249084605, 867.2348249084605, 864.2348249084605, 872.7348249084605, 783.3717702051356, 783.3717702051356, 784.3717702051356, 804.3717702051356, 783.3717702051356, 821.3406011388051, 852.8406011388051, 854.3406011388051, 821.8406011388051, 797.8406011388051, 849.3784705691619, 853.8784705691619, 832.8784705691619, 845.8784705691619, 836.3784705691619, 858.866748686255, 861.366748686255, 868.366748686255, 846.366748686255, 856.866748686255, 847.4833962858127, 838.4833962858127, 838.4833962858127, 874.4833962858127, 825.4833962858127, 944.2485000724519, 920.2485000724519, 957.7485000724519, 980.2485000724519, 949.7485000724519, 609.3478112600371, 584.8478112600371, 598.8478112600371, 612.8478112600371, 616.3478112600371, 855.5277405495416, 867.5277405495416, 879.0277405495416, 910.5277405495416, 901.0277405495416, -10289.522646614008, -10292.022646614008, -10292.022646614008, -10293.022646614008, -10288.522646614008, -10333.557517351393, -10333.557517351393, -10333.557517351393, -10333.557517351393, -10333.557517351393, -10363.358203464544, -10363.358203464544, -10363.358203464544, -10363.358203464544, -10363.358203464544, 498.03758445969044, 480.53758445969044, 480.53758445969044, 508.53758445969044, 512.0375844596904, -10346.792516876389, -10346.792516876389, -10346.792516876389, -10346.792516876389, -10346.792516876389, -10336.704877068783, -10336.704877068783, -10334.204877068783, -10336.704877068783, -10336.704877068783, 349.4680809733418, 342.4680809733418, 345.9680809733418, 349.4680809733418, 352.9680809733418, -10265.111596688603, -10279.111596688603, -10282.611596688603, -10286.111596688603, -10279.111596688603, 531.8411909590168, 517.8411909590168, 528.3411909590168, 531.8411909590168, 528.3411909590168, 462.4862886484992, 448.4862886484992, 451.9862886484992, 462.4862886484992, 465.9862886484992, 563.8281858857273, 560.3281858857273, 588.3281858857273, 570.8281858857273, 577.8281858857273, -10319.02342306272, -10319.02342306272, -10312.02342306272, -10319.02342306272, -10326.02342306272, 627.5984081943143, 596.0984081943143, 592.5984081943143, 582.0984081943143, 613.5984081943143, 622.4540311542918, 604.9540311542918, 583.9540311542918, 569.9540311542918, 629.4540311542918, -10260.97711407294, -10239.97711407294, -10253.97711407294, -10250.47711407294, -10253.97711407294, -10338.065320496686, -10338.065320496686, -10338.065320496686, -10338.065320496686, -10338.065320496686, 705.4084509008683, 673.9084509008683, 698.4084509008683, 705.4084509008683, 701.9084509008683, -10327.382350806573, -10327.382350806573, -10330.882350806573, -10327.382350806573, -10330.882350806573, -10352.890733288627, -10352.890733288627, -10356.390733288627, -10356.390733288627, -10356.390733288627, -10339.505215331967, -10339.505215331967, -10339.505215331967, -10339.505215331967, -10339.505215331967, 526.3670874939199, 522.8670874939199, 509.8670874939199, 503.8670874939198, 486.3670874939199, 461.8722137102867, 479.3722137102867, 482.8722137102867, 458.3722137102867, 507.3722137102867, 668.3683076877116, 701.3683076877116, 671.8683076877116, 675.3683076877116, 678.8683076877116, 487.7662328441579, 494.7662328441579, 505.2662328441579, 484.2662328441579, 501.7662328441579, -10329.116656961052, -10318.616656961052, -10322.116656961052, -10322.116656961052, -10329.116656961052, 710.4993718427731, 699.9993718427733, 671.9993718427733, 682.4993718427733, 710.4993718427733, 552.671849119046, 563.1718491190461, 556.1718491190461, 547.6718491190461, 558.1718491190461, 591.6761875082947, 539.1761875082947, 584.6761875082947, 581.1761875082947, 567.1761875082947, -10320.749902188349, -10317.249902188349, -10310.249902188349, -10313.749902188349, -10313.749902188349, 716.9589622610073, 695.9589622610073, 755.4589622610073, 692.4589622610073, 713.4589622610073, 580.125452614684, 634.625452614684, 604.125452614684, 574.125452614684, 583.625452614684, 818.1536150523324, 817.1536150523324, 785.6536150523324, 818.1536150523324, 795.1536150523324, 576.4688908291134, 576.4688908291134, 569.4688908291134, 565.9688908291134, 572.9688908291134, -10201.724601297594, -10183.224601297594, -10176.224601297594, -10177.224601297594, -10179.724601297594, 607.8624299820391, 590.3624299820391, 583.3624299820391, 586.8624299820391, 597.3624299820391, 1018.7193657334625, 974.2193657334625, 998.7193657334625, 992.2193657334625, 973.2193657334625, 936.3624780178735, 943.3624780178735, 954.8624780178735, 980.3624780178735, 939.8624780178735, -10122.525919925662, -10108.525919925662, -10115.525919925662, -10105.025919925662, -10119.025919925662, 721.6627445540253, 763.6627445540253, 749.6627445540253, 721.6627445540253, 753.1627445540253, 827.3545492553242, 869.3545492553242, 837.8545492553242, 830.8545492553242, 813.3545492553242]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 1.9757700993768967, &#x27;mean_inference_ms&#x27;: 0.5803132026572598, &#x27;mean_action_processing_ms&#x27;: 0.7312221542513175, &#x27;mean_env_wait_ms&#x27;: 0.1695161718845582, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0045549869537353516, &#x27;StateBufferConnector_ms&#x27;: 0.011849403381347656, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.13973164558410645}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 4606.776, &#x27;sample_time_ms&#x27;: 732.205, &#x27;learn_time_ms&#x27;: 3871.824, &#x27;learn_throughput&#x27;: 1033.105, &#x27;synch_weights_time_ms&#x27;: 2.112}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000000)\n",
      "\u001b[36m(RolloutWorker pid=80976)\u001b[0m 2024-11-23 20:30:08,481\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \u001b[32m [repeated 76x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=80976)\u001b[0m 2024-11-23 20:30:08,481\tWARNING multi_agent_env.py:246 -- action_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000001)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000002)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000003)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000004)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000005)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=80966)\u001b[0m 模型已保存，路径为：TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000006), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'shared_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.278447392757369, 'cur_kl_coeff': 0.0158203125, 'cur_lr': 0.0003, 'total_loss': 9.972784683975993, 'policy_loss': -0.004088834314194473, 'vf_loss': 9.976735165752942, 'vf_explained_var': 0.013945311496529398, 'kl': 0.008744058961412372, 'entropy': 0.6814882958237128, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 253.16455696202533, 'num_grad_updates_lifetime': 46610.5, 'diff_num_grad_updates_vs_sampler_policy': 789.5}}, 'num_env_steps_sampled': 120000, 'num_env_steps_trained': 120000, 'num_agent_steps_sampled': 600000, 'num_agent_steps_trained': 600000}, 'sampler_results': {'episode_reward_max': 4862.665814890702, 'episode_reward_min': -98568.93780462952, 'episode_reward_mean': -583.5856647230747, 'episode_len_mean': 200.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {'shared_policy': -19713.787560925884}, 'policy_reward_max': {'shared_policy': 994.3331629781403}, 'policy_reward_mean': {'shared_policy': -116.71713294461472}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [4073.9082630111566, 3267.2681128560316, 4477.283380878406, 3756.9274113895844, 2947.727329241101, 3615.2243416567394, 4531.548704205637, 1777.6261044153227, 3587.7664562380787, -51508.82769154805, 3443.8119421766314, 3733.755151695506, 3871.142418705726, 2766.896723840606, 3729.598361635497, 1950.6659864424655, 2112.6116900237043, 1655.772773946499, 4329.0861350991945, 2153.5373320222816, 3666.308902512756, 3943.0516267849453, 3796.651211679614, 4635.973344529453, 3780.7005730390874, 4037.1906651496283, 3624.0760026784756, 3519.8433776271195, 3901.9746142698077, 3587.027152003681, 3830.634584345069, 3318.268919796604, 3598.46553442424, 3990.0088669443057, 3935.6989369048633, 3986.5749216336717, 3735.056981843857, 4309.764393978885, 3634.0167910390355, 3900.1023154666113, 3129.844752541904, 3582.7268635031032, 4113.96389093052, 3597.1539845629018, 3636.2677118012725, 3391.928045722498, 3292.753586972277, 3745.012040921346, 3846.127153465511, 3627.672665412184, 4495.195920008532, -98568.93780462952, 3432.9896367067104, 4114.755886297267, 3790.4633994187834, -59661.95395132252, 3841.491751974662, 3605.1714236239122, -71481.15222674582, -70724.90717364322, 4419.625040430031, 4218.909117213668, -51888.40103842193, 3464.520215288905, 3647.1590759656365, 3812.0319947775724, 4266.998208356497, 3251.4432482762613, 4357.84359574722, 4119.696033296459, 4190.294287593122, 3935.7189285388986, 3775.2166916883616, 4088.0595976541736, 4039.7604450678105, 3678.4132978862467, 3881.161427018235, 3825.659663466713, 3897.509354071144, 3992.392590455083, 3957.2438437297033, 4795.522211348917, 2023.322976804026, 4253.459340506824, 4223.534890414616, 4177.278123065105, 3869.8237749090877, 3612.0726107898276, 1836.056226074857, 3821.641694756516, 2998.287036146562, 2683.808562276902, 4186.9823101449765, 3800.209778948103, 3581.856421121186, 4862.665814890702, 2821.4884799466117, 3959.687003246503, 3926.4111483431425, 3464.7813037521637], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_shared_policy_reward': [833.0816526022313, 815.5816526022313, 818.0816526022313, 806.5816526022313, 800.5816526022313, 677.9536225712066, 660.4536225712066, 646.4536225712066, 646.4536225712066, 635.9536225712066, 883.4566761756812, 889.9566761756812, 921.9566761756812, 903.9566761756812, 877.9566761756812, 721.2854822779168, 763.2854822779168, 745.7854822779168, 777.2854822779168, 749.2854822779168, 596.5454658482205, 551.0454658482205, 607.0454658482205, 596.5454658482205, 596.5454658482205, 710.244868331348, 709.244868331348, 722.744868331348, 746.244868331348, 726.744868331348, 900.9097408411274, 916.4097408411274, 927.9097408411274, 901.4097408411274, 884.9097408411274, 341.52522088306443, 348.52522088306443, 359.02522088306443, 355.52522088306443, 373.02522088306443, 697.2532912476156, 721.7532912476156, 718.2532912476156, 732.2532912476156, 718.2532912476156, -10296.865538309612, -10307.365538309612, -10296.865538309612, -10300.365538309612, -10307.365538309612, 706.9623884353259, 727.9623884353259, 671.9623884353259, 678.9623884353259, 657.9623884353259, 750.9510303391011, 743.9510303391011, 736.9510303391011, 736.9510303391011, 764.9510303391011, 805.3284837411454, 758.8284837411454, 777.3284837411454, 778.8284837411454, 750.8284837411454, 566.6793447681212, 552.6793447681212, 538.6793447681212, 549.1793447681212, 559.6793447681212, 761.3196723270994, 767.8196723270994, 762.3196723270994, 731.8196723270994, 706.3196723270994, 385.9331972884926, 392.9331972884926, 382.4331972884926, 389.4331972884926, 399.9331972884926, 431.62233800473985, 424.62233800473985, 403.62233800473985, 431.62233800473985, 421.12233800473985, 341.6545547893002, 327.6545547893002, 331.1545547893002, 317.1545547893002, 338.1545547893002, 828.0172270198389, 865.0172270198389, 870.0172270198389, 873.0172270198389, 893.0172270198389, 432.807466404457, 404.807466404457, 418.807466404457, 460.807466404457, 436.307466404457, 750.8617805025513, 752.3617805025513, 716.8617805025513, 719.8617805025513, 726.3617805025513, 830.8103253569891, 781.310325356989, 800.310325356989, 763.310325356989, 767.310325356989, 750.7302423359229, 761.2302423359229, 782.2302423359229, 738.7302423359229, 763.7302423359229, 932.8946689058907, 900.3946689058907, 941.8946689058907, 963.3946689058907, 897.3946689058907, 810.4401146078176, 728.9401146078176, 740.4401146078176, 735.9401146078176, 764.9401146078176, 849.8381330299256, 805.3381330299256, 793.8381330299256, 803.8381330299256, 784.3381330299256, 719.0152005356952, 725.5152005356952, 717.0152005356952, 720.5152005356952, 742.0152005356952, 709.3686755254239, 689.3686755254239, 719.8686755254239, 689.8686755254239, 711.3686755254239, 790.9949228539615, 772.9949228539615, 807.4949228539615, 783.9949228539615, 746.4949228539615, 730.3054304007361, 719.8054304007361, 717.3054304007361, 738.3054304007361, 681.3054304007361, 756.9269168690138, 798.4269168690138, 763.4269168690138, 744.9269168690138, 766.9269168690138, 650.9537839593211, 646.4537839593211, 668.4537839593211, 646.4537839593211, 705.9537839593211, 731.693106884848, 713.193106884848, 715.193106884848, 746.693106884848, 691.693106884848, 815.4017733888611, 777.4017733888611, 782.9017733888611, 781.9017733888611, 832.4017733888611, 806.4397873809726, 801.9397873809726, 760.9397873809726, 763.9397873809726, 802.4397873809726, 806.2149843267343, 803.2149843267343, 814.7149843267343, 785.2149843267343, 777.2149843267343, 730.7113963687715, 758.7113963687715, 761.2113963687715, 729.2113963687715, 755.2113963687715, 855.5528787957769, 868.0528787957769, 851.5528787957769, 869.5528787957769, 865.0528787957769, 758.1033582078071, 742.6033582078071, 736.1033582078071, 672.6033582078071, 724.6033582078071, 790.4204630933222, 796.4204630933222, 750.9204630933222, 807.9204630933222, 754.4204630933222, 626.6689505083812, 640.6689505083812, 616.1689505083812, 616.1689505083812, 630.1689505083812, 694.2453727006206, 730.7453727006206, 745.7453727006206, 691.7453727006206, 720.2453727006206, 800.992778186104, 834.992778186104, 845.492778186104, 804.492778186104, 827.992778186104, 698.8307969125804, 723.8307969125804, 712.3307969125804, 741.8307969125804, 720.3307969125804, 735.2535423602545, 725.2535423602545, 737.2535423602545, 698.2535423602545, 740.2535423602545, 666.8856091444997, 672.8856091444997, 683.8856091444997, 686.8856091444997, 681.3856091444997, 647.2507173944555, 636.7507173944555, 665.7507173944555, 678.2507173944555, 664.7507173944555, 768.7024081842692, 743.2024081842691, 743.2024081842692, 749.2024081842692, 740.7024081842692, 749.6254306931023, 795.1254306931023, 777.6254306931023, 756.6254306931023, 767.1254306931023, 729.1345330824369, 706.6345330824369, 736.6345330824369, 730.6345330824369, 724.6345330824369, 921.1391840017063, 895.1391840017063, 901.1391840017063, 900.6391840017063, 877.1391840017063, -19713.787560925884, -19713.787560925884, -19713.787560925884, -19713.787560925884, -19713.787560925884, 647.3979273413421, 685.8979273413421, 734.8979273413421, 661.3979273413421, 703.3979273413421, 844.1511772594536, 837.1511772594536, 810.1511772594536, 818.6511772594536, 804.6511772594536, 770.6926798837567, 760.1926798837567, 763.6926798837567, 756.6926798837567, 739.1926798837567, -11932.390790264502, -11932.390790264502, -11932.390790264502, -11932.390790264502, -11932.390790264502, 808.4983503949323, 772.9983503949323, 769.9983503949323, 755.9983503949323, 733.9983503949323, 741.7342847247824, 677.7342847247824, 740.7342847247824, 741.7342847247824, 703.2342847247824, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14144.981434728648, -14144.981434728648, -14144.981434728648, -14144.981434728648, -14144.981434728648, 880.325008086006, 873.825008086006, 883.825008086006, 883.825008086006, 897.825008086006, 828.5818234427336, 812.5818234427336, 854.0818234427336, 863.0818234427336, 860.5818234427336, -10377.680207684387, -10377.680207684387, -10377.680207684387, -10377.680207684387, -10377.680207684387, 729.304043057781, 662.804043057781, 683.804043057781, 690.804043057781, 697.804043057781, 697.1318151931273, 756.6318151931273, 744.6318151931273, 710.6318151931273, 738.1318151931273, 769.3063989555144, 778.3063989555144, 751.8063989555144, 731.8063989555144, 780.8063989555144, 849.1996416712994, 836.1996416712994, 868.1996416712994, 861.6996416712994, 851.6996416712994, 641.8886496552519, 641.8886496552519, 655.8886496552519, 641.8886496552519, 669.8886496552519, 842.8687191494441, 881.8687191494441, 880.3687191494441, 888.3687191494441, 864.3687191494441, 796.1392066592919, 826.6392066592919, 836.1392066592919, 818.1392066592919, 842.6392066592919, 817.7588575186244, 848.7588575186244, 833.2588575186244, 848.2588575186244, 842.2588575186244, 796.9437857077796, 775.9437857077796, 779.4437857077796, 821.4437857077796, 761.9437857077796, 752.0433383376723, 773.0433383376723, 769.5433383376723, 717.0433383376723, 763.5433383376723, 810.6119195308347, 838.1119195308347, 812.1119195308347, 813.1119195308347, 814.1119195308347, 769.452089013562, 822.452089013562, 805.952089013562, 814.952089013562, 826.952089013562, 706.2826595772492, 734.2826595772492, 744.7826595772492, 744.7826595772492, 748.2826595772492, 751.9322854036469, 772.4322854036469, 794.9322854036469, 771.9322854036469, 789.9322854036469, 759.4319326933426, 770.4319326933426, 745.9319326933426, 755.4319326933426, 794.4319326933426, 793.8018708142288, 784.8018708142288, 750.3018708142288, 780.8018708142288, 787.8018708142288, 758.1785180910165, 787.1785180910165, 811.1785180910165, 799.1785180910165, 836.6785180910165, 787.2487687459407, 811.7487687459407, 804.7487687459407, 755.7487687459407, 797.7487687459407, 946.9044422697834, 938.9044422697834, 984.4044422697834, 993.9044422697834, 931.4044422697834, 411.664595360805, 429.164595360805, 390.664595360805, 383.664595360805, 408.164595360805, 845.4918681013651, 868.9918681013651, 831.4918681013651, 847.9918681013651, 859.4918681013651, 844.5069780829232, 839.0069780829232, 857.5069780829232, 876.0069780829232, 806.5069780829232, 857.3556246130211, 807.355624613021, 851.3556246130211, 845.8556246130211, 815.355624613021, 803.3647549818177, 768.3647549818177, 768.3647549818177, 768.3647549818177, 761.3647549818177, 737.1145221579652, 684.6145221579652, 716.1145221579652, 761.6145221579652, 712.6145221579652, 379.11124521497106, 379.11124521497106, 375.61124521497106, 337.11124521497106, 365.11124521497106, 772.7283389513033, 793.7283389513033, 744.7283389513033, 755.2283389513033, 755.2283389513033, 587.0574072293128, 618.5574072293128, 604.5574072293128, 608.0574072293128, 580.0574072293128, 534.6617124553802, 520.6617124553802, 552.1617124553802, 545.1617124553802, 531.1617124553802, 838.9964620289954, 820.4964620289954, 825.9964620289954, 852.9964620289954, 848.4964620289954, 771.2419557896205, 767.7419557896205, 725.7419557896205, 750.2419557896205, 785.2419557896205, 728.9712842242371, 697.4712842242371, 718.4712842242371, 711.4712842242371, 725.4712842242371, 948.3331629781403, 978.8331629781403, 994.3331629781403, 979.8331629781403, 961.3331629781403, 564.2976959893225, 529.2976959893225, 585.2976959893225, 567.7976959893225, 574.7976959893225, 798.9374006493006, 788.4374006493006, 784.9374006493006, 798.9374006493006, 788.4374006493006, 783.8822296686284, 754.8822296686284, 768.8822296686284, 815.8822296686284, 802.8822296686284, 727.2562607504331, 702.7562607504331, 667.7562607504331, 657.2562607504331, 709.7562607504331]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.9584924717457932, 'mean_inference_ms': 0.5751600347514941, 'mean_action_processing_ms': 0.7264722748066302, 'mean_env_wait_ms': 0.169260104554251, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004580020904541016, 'StateBufferConnector_ms': 0.012148857116699219, 'ViewRequirementAgentConnector_ms': 0.14263558387756348}}, 'episode_reward_max': 4862.665814890702, 'episode_reward_min': -98568.93780462952, 'episode_reward_mean': -583.5856647230747, 'episode_len_mean': 200.0, 'episodes_this_iter': 20, 'policy_reward_min': {'shared_policy': -19713.787560925884}, 'policy_reward_max': {'shared_policy': 994.3331629781403}, 'policy_reward_mean': {'shared_policy': -116.71713294461472}, 'hist_stats': {'episode_reward': [4073.9082630111566, 3267.2681128560316, 4477.283380878406, 3756.9274113895844, 2947.727329241101, 3615.2243416567394, 4531.548704205637, 1777.6261044153227, 3587.7664562380787, -51508.82769154805, 3443.8119421766314, 3733.755151695506, 3871.142418705726, 2766.896723840606, 3729.598361635497, 1950.6659864424655, 2112.6116900237043, 1655.772773946499, 4329.0861350991945, 2153.5373320222816, 3666.308902512756, 3943.0516267849453, 3796.651211679614, 4635.973344529453, 3780.7005730390874, 4037.1906651496283, 3624.0760026784756, 3519.8433776271195, 3901.9746142698077, 3587.027152003681, 3830.634584345069, 3318.268919796604, 3598.46553442424, 3990.0088669443057, 3935.6989369048633, 3986.5749216336717, 3735.056981843857, 4309.764393978885, 3634.0167910390355, 3900.1023154666113, 3129.844752541904, 3582.7268635031032, 4113.96389093052, 3597.1539845629018, 3636.2677118012725, 3391.928045722498, 3292.753586972277, 3745.012040921346, 3846.127153465511, 3627.672665412184, 4495.195920008532, -98568.93780462952, 3432.9896367067104, 4114.755886297267, 3790.4633994187834, -59661.95395132252, 3841.491751974662, 3605.1714236239122, -71481.15222674582, -70724.90717364322, 4419.625040430031, 4218.909117213668, -51888.40103842193, 3464.520215288905, 3647.1590759656365, 3812.0319947775724, 4266.998208356497, 3251.4432482762613, 4357.84359574722, 4119.696033296459, 4190.294287593122, 3935.7189285388986, 3775.2166916883616, 4088.0595976541736, 4039.7604450678105, 3678.4132978862467, 3881.161427018235, 3825.659663466713, 3897.509354071144, 3992.392590455083, 3957.2438437297033, 4795.522211348917, 2023.322976804026, 4253.459340506824, 4223.534890414616, 4177.278123065105, 3869.8237749090877, 3612.0726107898276, 1836.056226074857, 3821.641694756516, 2998.287036146562, 2683.808562276902, 4186.9823101449765, 3800.209778948103, 3581.856421121186, 4862.665814890702, 2821.4884799466117, 3959.687003246503, 3926.4111483431425, 3464.7813037521637], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_shared_policy_reward': [833.0816526022313, 815.5816526022313, 818.0816526022313, 806.5816526022313, 800.5816526022313, 677.9536225712066, 660.4536225712066, 646.4536225712066, 646.4536225712066, 635.9536225712066, 883.4566761756812, 889.9566761756812, 921.9566761756812, 903.9566761756812, 877.9566761756812, 721.2854822779168, 763.2854822779168, 745.7854822779168, 777.2854822779168, 749.2854822779168, 596.5454658482205, 551.0454658482205, 607.0454658482205, 596.5454658482205, 596.5454658482205, 710.244868331348, 709.244868331348, 722.744868331348, 746.244868331348, 726.744868331348, 900.9097408411274, 916.4097408411274, 927.9097408411274, 901.4097408411274, 884.9097408411274, 341.52522088306443, 348.52522088306443, 359.02522088306443, 355.52522088306443, 373.02522088306443, 697.2532912476156, 721.7532912476156, 718.2532912476156, 732.2532912476156, 718.2532912476156, -10296.865538309612, -10307.365538309612, -10296.865538309612, -10300.365538309612, -10307.365538309612, 706.9623884353259, 727.9623884353259, 671.9623884353259, 678.9623884353259, 657.9623884353259, 750.9510303391011, 743.9510303391011, 736.9510303391011, 736.9510303391011, 764.9510303391011, 805.3284837411454, 758.8284837411454, 777.3284837411454, 778.8284837411454, 750.8284837411454, 566.6793447681212, 552.6793447681212, 538.6793447681212, 549.1793447681212, 559.6793447681212, 761.3196723270994, 767.8196723270994, 762.3196723270994, 731.8196723270994, 706.3196723270994, 385.9331972884926, 392.9331972884926, 382.4331972884926, 389.4331972884926, 399.9331972884926, 431.62233800473985, 424.62233800473985, 403.62233800473985, 431.62233800473985, 421.12233800473985, 341.6545547893002, 327.6545547893002, 331.1545547893002, 317.1545547893002, 338.1545547893002, 828.0172270198389, 865.0172270198389, 870.0172270198389, 873.0172270198389, 893.0172270198389, 432.807466404457, 404.807466404457, 418.807466404457, 460.807466404457, 436.307466404457, 750.8617805025513, 752.3617805025513, 716.8617805025513, 719.8617805025513, 726.3617805025513, 830.8103253569891, 781.310325356989, 800.310325356989, 763.310325356989, 767.310325356989, 750.7302423359229, 761.2302423359229, 782.2302423359229, 738.7302423359229, 763.7302423359229, 932.8946689058907, 900.3946689058907, 941.8946689058907, 963.3946689058907, 897.3946689058907, 810.4401146078176, 728.9401146078176, 740.4401146078176, 735.9401146078176, 764.9401146078176, 849.8381330299256, 805.3381330299256, 793.8381330299256, 803.8381330299256, 784.3381330299256, 719.0152005356952, 725.5152005356952, 717.0152005356952, 720.5152005356952, 742.0152005356952, 709.3686755254239, 689.3686755254239, 719.8686755254239, 689.8686755254239, 711.3686755254239, 790.9949228539615, 772.9949228539615, 807.4949228539615, 783.9949228539615, 746.4949228539615, 730.3054304007361, 719.8054304007361, 717.3054304007361, 738.3054304007361, 681.3054304007361, 756.9269168690138, 798.4269168690138, 763.4269168690138, 744.9269168690138, 766.9269168690138, 650.9537839593211, 646.4537839593211, 668.4537839593211, 646.4537839593211, 705.9537839593211, 731.693106884848, 713.193106884848, 715.193106884848, 746.693106884848, 691.693106884848, 815.4017733888611, 777.4017733888611, 782.9017733888611, 781.9017733888611, 832.4017733888611, 806.4397873809726, 801.9397873809726, 760.9397873809726, 763.9397873809726, 802.4397873809726, 806.2149843267343, 803.2149843267343, 814.7149843267343, 785.2149843267343, 777.2149843267343, 730.7113963687715, 758.7113963687715, 761.2113963687715, 729.2113963687715, 755.2113963687715, 855.5528787957769, 868.0528787957769, 851.5528787957769, 869.5528787957769, 865.0528787957769, 758.1033582078071, 742.6033582078071, 736.1033582078071, 672.6033582078071, 724.6033582078071, 790.4204630933222, 796.4204630933222, 750.9204630933222, 807.9204630933222, 754.4204630933222, 626.6689505083812, 640.6689505083812, 616.1689505083812, 616.1689505083812, 630.1689505083812, 694.2453727006206, 730.7453727006206, 745.7453727006206, 691.7453727006206, 720.2453727006206, 800.992778186104, 834.992778186104, 845.492778186104, 804.492778186104, 827.992778186104, 698.8307969125804, 723.8307969125804, 712.3307969125804, 741.8307969125804, 720.3307969125804, 735.2535423602545, 725.2535423602545, 737.2535423602545, 698.2535423602545, 740.2535423602545, 666.8856091444997, 672.8856091444997, 683.8856091444997, 686.8856091444997, 681.3856091444997, 647.2507173944555, 636.7507173944555, 665.7507173944555, 678.2507173944555, 664.7507173944555, 768.7024081842692, 743.2024081842691, 743.2024081842692, 749.2024081842692, 740.7024081842692, 749.6254306931023, 795.1254306931023, 777.6254306931023, 756.6254306931023, 767.1254306931023, 729.1345330824369, 706.6345330824369, 736.6345330824369, 730.6345330824369, 724.6345330824369, 921.1391840017063, 895.1391840017063, 901.1391840017063, 900.6391840017063, 877.1391840017063, -19713.787560925884, -19713.787560925884, -19713.787560925884, -19713.787560925884, -19713.787560925884, 647.3979273413421, 685.8979273413421, 734.8979273413421, 661.3979273413421, 703.3979273413421, 844.1511772594536, 837.1511772594536, 810.1511772594536, 818.6511772594536, 804.6511772594536, 770.6926798837567, 760.1926798837567, 763.6926798837567, 756.6926798837567, 739.1926798837567, -11932.390790264502, -11932.390790264502, -11932.390790264502, -11932.390790264502, -11932.390790264502, 808.4983503949323, 772.9983503949323, 769.9983503949323, 755.9983503949323, 733.9983503949323, 741.7342847247824, 677.7342847247824, 740.7342847247824, 741.7342847247824, 703.2342847247824, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14296.23044534917, -14144.981434728648, -14144.981434728648, -14144.981434728648, -14144.981434728648, -14144.981434728648, 880.325008086006, 873.825008086006, 883.825008086006, 883.825008086006, 897.825008086006, 828.5818234427336, 812.5818234427336, 854.0818234427336, 863.0818234427336, 860.5818234427336, -10377.680207684387, -10377.680207684387, -10377.680207684387, -10377.680207684387, -10377.680207684387, 729.304043057781, 662.804043057781, 683.804043057781, 690.804043057781, 697.804043057781, 697.1318151931273, 756.6318151931273, 744.6318151931273, 710.6318151931273, 738.1318151931273, 769.3063989555144, 778.3063989555144, 751.8063989555144, 731.8063989555144, 780.8063989555144, 849.1996416712994, 836.1996416712994, 868.1996416712994, 861.6996416712994, 851.6996416712994, 641.8886496552519, 641.8886496552519, 655.8886496552519, 641.8886496552519, 669.8886496552519, 842.8687191494441, 881.8687191494441, 880.3687191494441, 888.3687191494441, 864.3687191494441, 796.1392066592919, 826.6392066592919, 836.1392066592919, 818.1392066592919, 842.6392066592919, 817.7588575186244, 848.7588575186244, 833.2588575186244, 848.2588575186244, 842.2588575186244, 796.9437857077796, 775.9437857077796, 779.4437857077796, 821.4437857077796, 761.9437857077796, 752.0433383376723, 773.0433383376723, 769.5433383376723, 717.0433383376723, 763.5433383376723, 810.6119195308347, 838.1119195308347, 812.1119195308347, 813.1119195308347, 814.1119195308347, 769.452089013562, 822.452089013562, 805.952089013562, 814.952089013562, 826.952089013562, 706.2826595772492, 734.2826595772492, 744.7826595772492, 744.7826595772492, 748.2826595772492, 751.9322854036469, 772.4322854036469, 794.9322854036469, 771.9322854036469, 789.9322854036469, 759.4319326933426, 770.4319326933426, 745.9319326933426, 755.4319326933426, 794.4319326933426, 793.8018708142288, 784.8018708142288, 750.3018708142288, 780.8018708142288, 787.8018708142288, 758.1785180910165, 787.1785180910165, 811.1785180910165, 799.1785180910165, 836.6785180910165, 787.2487687459407, 811.7487687459407, 804.7487687459407, 755.7487687459407, 797.7487687459407, 946.9044422697834, 938.9044422697834, 984.4044422697834, 993.9044422697834, 931.4044422697834, 411.664595360805, 429.164595360805, 390.664595360805, 383.664595360805, 408.164595360805, 845.4918681013651, 868.9918681013651, 831.4918681013651, 847.9918681013651, 859.4918681013651, 844.5069780829232, 839.0069780829232, 857.5069780829232, 876.0069780829232, 806.5069780829232, 857.3556246130211, 807.355624613021, 851.3556246130211, 845.8556246130211, 815.355624613021, 803.3647549818177, 768.3647549818177, 768.3647549818177, 768.3647549818177, 761.3647549818177, 737.1145221579652, 684.6145221579652, 716.1145221579652, 761.6145221579652, 712.6145221579652, 379.11124521497106, 379.11124521497106, 375.61124521497106, 337.11124521497106, 365.11124521497106, 772.7283389513033, 793.7283389513033, 744.7283389513033, 755.2283389513033, 755.2283389513033, 587.0574072293128, 618.5574072293128, 604.5574072293128, 608.0574072293128, 580.0574072293128, 534.6617124553802, 520.6617124553802, 552.1617124553802, 545.1617124553802, 531.1617124553802, 838.9964620289954, 820.4964620289954, 825.9964620289954, 852.9964620289954, 848.4964620289954, 771.2419557896205, 767.7419557896205, 725.7419557896205, 750.2419557896205, 785.2419557896205, 728.9712842242371, 697.4712842242371, 718.4712842242371, 711.4712842242371, 725.4712842242371, 948.3331629781403, 978.8331629781403, 994.3331629781403, 979.8331629781403, 961.3331629781403, 564.2976959893225, 529.2976959893225, 585.2976959893225, 567.7976959893225, 574.7976959893225, 798.9374006493006, 788.4374006493006, 784.9374006493006, 798.9374006493006, 788.4374006493006, 783.8822296686284, 754.8822296686284, 768.8822296686284, 815.8822296686284, 802.8822296686284, 727.2562607504331, 702.7562607504331, 667.7562607504331, 657.2562607504331, 709.7562607504331]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.9584924717457932, 'mean_inference_ms': 0.5751600347514941, 'mean_action_processing_ms': 0.7264722748066302, 'mean_env_wait_ms': 0.169260104554251, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004580020904541016, 'StateBufferConnector_ms': 0.012148857116699219, 'ViewRequirementAgentConnector_ms': 0.14263558387756348}, 'num_healthy_workers': 4, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 600000, 'num_agent_steps_trained': 600000, 'num_env_steps_sampled': 120000, 'num_env_steps_trained': 120000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 861.549977112244, 'num_env_steps_trained_throughput_per_sec': 861.549977112244, 'timesteps_total': 120000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 600000, 'timers': {'training_iteration_time_ms': 4639.986, 'sample_time_ms': 701.859, 'learn_time_ms': 3935.461, 'learn_throughput': 1016.399, 'synch_weights_time_ms': 2.018}, 'counters': {'num_env_steps_sampled': 120000, 'num_env_steps_trained': 120000, 'num_agent_steps_sampled': 600000, 'num_agent_steps_trained': 600000}, 'done': False, 'episodes_total': 600, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-11-23_20-32-28', 'timestamp': 1732365148, 'time_this_iter_s': 4.645128965377808, 'time_total_s': 138.89846897125244, 'pid': 80966, 'hostname': 'Jareds-Mac-miniM2.local', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'env', 'env_config': {'num_agents': 5}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 5, 'enable_connectors': True, '_env_to_module_connector': None, '_module_to_env_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'validate_workers_after_construction': True, 'compress_observations': False, 'sampler_perf_stats_ema_coef': None, 'sample_async': -1, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'gamma': 0.99, 'lr': 0.0003, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function shared_policy_mapping_fn at 0x15b668700>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'custom_async_evaluation_function': None, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_enable_new_api_stack': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 20, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, '__stdout_file__': None, '__stderr_file__': None, 'lambda': 1.0, 'input': 'sampler', 'policies': {'shared_policy': (None, Box(-inf, inf, (5,), float32), Discrete(2), {})}, 'callbacks': <class '__main__.SaveOnMaxRewardCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 4}, 'time_since_restore': 138.89846897125244, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 17.2, 'ram_util_percent': 75.52857142857144}})，最大奖励：5080.126925187507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000007)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000008)\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000009)\n",
      "2024-11-23 20:34:01,750\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-11-23 20:34:01,755\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/Users/cyj/ray_results/PPO_2024-11-23_20-29-59' in 0.0225s.\n",
      "\u001b[36m(PPO pid=80966)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/cyj/ray_results/PPO_2024-11-23_20-29-59/PPO_env_a7d11_00000_0_2024-11-23_20-29-59/checkpoint_000010)\n",
      "2024-11-23 20:34:01,928\tINFO tune.py:1048 -- Total run time: 242.18 seconds (241.98 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "import os\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "# 定义环境类\n",
    "class MAEnvironment(MultiAgentEnv):\n",
    "    def __init__(self, num_agents=5, num_iterations=200, dt=0.1):\n",
    "        super().__init__()  # 调用父类初始化\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [\"agent_\" + str(i) for i in range(num_agents)]\n",
    "        self.agent_name_mapping = dict(zip(self.agents, list(range(num_agents))))\n",
    "        self._agent_ids = set(self.agents)  # 添加 _agent_ids 属性\n",
    "\n",
    "        # 初始化其他属性\n",
    "        self.num_iterations = num_iterations\n",
    "        self.dt = dt\n",
    "        self.current_iteration = 0\n",
    "\n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "\n",
    "        self.epsilon = 0.005\n",
    "        self.time_to_reach_epsilon = None\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.max_obs_size = self.compute_max_obs_size()\n",
    "    \n",
    "    def compute_max_obs_size(self):\n",
    "        max_neighbors = max(len(agent.neighbors) for agent in self.agent_objs)\n",
    "        return 1 + max_neighbors\n",
    "    \n",
    "    def init_neighbors(self):\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[1])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[3].add_neighbor(self.agent_objs[4])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "        self.current_iteration = 0\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.time_to_reach_epsilon = None\n",
    "        \n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}  # 返回额外的 per-agent 信息字典\n",
    "        return observations, infos\n",
    "\n",
    "    # 统一的观测空间\n",
    "    # def get_observation(self, agent):\n",
    "    #     agent_index = self.agent_name_mapping[agent]\n",
    "    #     agent_obj = self.agent_objs[agent_index]\n",
    "    #     neighbors_positions = [neighbor.position for neighbor in agent_obj.neighbors]\n",
    "    #     obs = np.array([agent_obj.position] + neighbors_positions, dtype=np.float32)\n",
    "        \n",
    "    #     # 填充观测到最大观测大小\n",
    "    #     if len(obs) < self.max_obs_size:\n",
    "    #         padding = np.zeros(self.max_obs_size - len(obs))\n",
    "    #         obs = np.concatenate([obs, padding])\n",
    "        \n",
    "    #     # 不进行裁剪\n",
    "    #     return obs\n",
    "\n",
    "    # # 不同的观测空间\n",
    "    def get_observation(self, agent):\n",
    "        agent_index = self.agent_name_mapping[agent]\n",
    "        agent_obj = self.agent_objs[agent_index]\n",
    "        neighbors_positions = [neighbor.position for neighbor in agent_obj.neighbors]\n",
    "        obs = np.array([agent_obj.position] + neighbors_positions, dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def compute_average_position_difference(self):\n",
    "        total_difference = 0\n",
    "        count = 0\n",
    "        for i, agent_i in enumerate(self.agent_objs):\n",
    "            for j, agent_j in enumerate(self.agent_objs):\n",
    "                if i < j:\n",
    "                    total_difference += abs(agent_i.position - agent_j.position)\n",
    "                    count += 1\n",
    "        if count > 0:\n",
    "            return total_difference / count\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        triggers = np.array([action_dict.get(agent, 0) for agent in self.agents])  # 确保访问安全\n",
    "        trigger_count = np.sum(triggers)\n",
    "        self.total_trigger_count += trigger_count\n",
    "\n",
    "        for i, agent in enumerate(self.agent_objs):\n",
    "            agent.update_position(self.current_iteration, self.dt, triggers[i])\n",
    "\n",
    "        self.all_within_epsilon = all(all(abs(agent.position - neighbor.position) < self.epsilon for neighbor in agent.neighbors) for agent in self.agent_objs)\n",
    "\n",
    "        if self.all_within_epsilon:\n",
    "            if self.epsilon_violated:\n",
    "                self.time_to_reach_epsilon = self.current_iteration\n",
    "                self.epsilon_violated = False\n",
    "                self.time_to_reach_epsilon_changes += 1\n",
    "        else:\n",
    "            self.epsilon_violated = True\n",
    "            self.time_to_reach_epsilon = None\n",
    "        \n",
    "        self.current_iteration += 1\n",
    "        terminated = self.current_iteration >= self.num_iterations\n",
    "\n",
    "        # 根据时间步调整奖励逻辑\n",
    "        early_phase = self.current_iteration <= self.num_iterations * 0.25\n",
    "\n",
    "\n",
    "        rewards = {}\n",
    "        if not terminated:\n",
    "            average_position_difference = self.compute_average_position_difference()\n",
    "            # for agent in self.agents:\n",
    "            #     if self.all_within_epsilon:\n",
    "            #         rewards[agent] = 10 if action_dict.get(agent, 0) == 0 else 0  # 动作为0奖励，1惩罚\n",
    "            #     else:\n",
    "            #         rewards[agent] = - 10 * np.abs(average_position_difference)\n",
    "\n",
    "            for agent in self.agents:\n",
    "                if early_phase:\n",
    "                    # 在前 25% 的时间步，奖励更加注重位置一致性，减少对触发的惩罚\n",
    "                    if self.all_within_epsilon:\n",
    "                        rewards[agent] = 2 if action_dict.get(agent, 0) == 0 else 1\n",
    "                    else:\n",
    "                        rewards[agent] = -1 -10 * np.abs(average_position_difference)\n",
    "                else:\n",
    "                    # 在后 75% 的时间步，更加注重减少触发次数\n",
    "                    if self.all_within_epsilon:\n",
    "                        rewards[agent] = 3 if action_dict.get(agent, 0) == 0 else -0.5\n",
    "                    else:\n",
    "                        rewards[agent] = -1.5 - 10 * np.abs(average_position_difference)\n",
    "        else:\n",
    "            if self.time_to_reach_epsilon is not None:\n",
    "                global_reward = 1250 - self.time_to_reach_epsilon - self.total_trigger_count\n",
    "            else:\n",
    "                global_reward = -10000\n",
    "            for agent in self.agents:\n",
    "                rewards[agent] = global_reward\n",
    "\n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        terminateds = {agent: terminated for agent in self.agents}\n",
    "        terminateds[\"__all__\"] = terminated\n",
    "        truncateds = {agent: False for agent in self.agents}  # 无需提前结束\n",
    "        truncateds[\"__all__\"] = False\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        positions = [agent.position for agent in self.agent_objs]\n",
    "        print(f\"Positions: {positions}\")\n",
    "    \n",
    "    # 统一大小的观测空间\n",
    "    # def observation_space(self, agent):\n",
    "    #     return spaces.Box(low=-np.inf, high=np.inf, shape=(self.max_obs_size,), dtype=np.float32)\n",
    "    \n",
    "    def observation_space(self, agent):\n",
    "        num_neighbors = len(self.agent_objs[self.agent_name_mapping[agent]].neighbors)\n",
    "        obs_size = 1 + num_neighbors  # 自身位置 + 邻居数量\n",
    "        return spaces.Box(low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float32)\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "    class Agent:\n",
    "        def __init__(self, initial_position, index):\n",
    "            self.position = initial_position\n",
    "            self.index = index\n",
    "            self.neighbors = []\n",
    "            self.last_broadcast_position = self.position\n",
    "            self.trigger_points = []\n",
    "            self.u_i = 0\n",
    "\n",
    "        def add_neighbor(self, neighbor):\n",
    "            if neighbor not in self.neighbors:\n",
    "                self.neighbors.append(neighbor)\n",
    "                neighbor.neighbors.append(self)\n",
    "\n",
    "        def update_position(self, t, dt, trigger):\n",
    "            if trigger == 1 or t == 0:\n",
    "                self.u_i = -sum((self.last_broadcast_position - neighbor.last_broadcast_position) for neighbor in self.neighbors)\n",
    "                self.position += self.u_i * dt\n",
    "                self.last_broadcast_position = self.position\n",
    "                self.trigger_points.append((t, self.position))\n",
    "            else:\n",
    "                self.position += self.u_i * dt\n",
    "\n",
    "# 环境创建函数\n",
    "def env_creator(config):\n",
    "    return MAEnvironment(num_agents=config.get(\"num_agents\", 5))\n",
    "\n",
    "# 注册环境\n",
    "register_env(\"env\", lambda config: MAEnvironment(num_agents=config.get(\"num_agents\", 5)))\n",
    "print(\"环境注册成功\")\n",
    "\n",
    "\n",
    "# 定义共享策略的映射函数\n",
    "def shared_policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    return \"shared_policy\"\n",
    "\n",
    "# 启动 Ray\n",
    "ray.shutdown() \n",
    "ray.init(local_mode=False)\n",
    "\n",
    "# 配置\n",
    "config = {\n",
    "    \"env\": \"env\",  # 使用注册的环境名\n",
    "    \"env_config\": {\n",
    "        \"num_agents\": 5,  # 传递环境的配置参数\n",
    "    },\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"shared_policy\": (None,  # 使用默认模型\n",
    "                              env_creator({\"num_agents\": 5}).observation_space(\"agent_0\"),  # 观测空间\n",
    "                              env_creator({\"num_agents\": 5}).action_space(\"agent_0\"),  # 动作空间\n",
    "                              {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": shared_policy_mapping_fn,  # 使用共享策略映射\n",
    "    },\n",
    "    \"framework\": \"torch\",  # 使用 \"torch\" 或 \"tf\"\n",
    "    \"num_workers\": 4,  # 使用的工作线程数\n",
    "    \"num_envs_per_worker\": 5,\n",
    "    \"train_batch_size\": 4000, #每次训练时使用的总样本数\n",
    "    \"sgd_minibatch_size\": 256,\n",
    "    \"lr\": 0.0003,\n",
    "    \"num_sgd_iter\": 20,\n",
    "}\n",
    "\n",
    "print(\"开始训练\")\n",
    "\n",
    "class SaveOnMaxRewardCallback(DefaultCallbacks):\n",
    "    def __init__(self, reward_threshold=5000, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reward_threshold = reward_threshold  # 奖励阈值\n",
    "        self.saved_checkpoint = False  # 确保只保存一次模型\n",
    "\n",
    "    def on_train_result(self, *, algorithm, result, **kwargs):\n",
    "        \"\"\"在每次训练结束时调用，检查最大奖励并保存模型。\"\"\"\n",
    "        max_reward = result[\"episode_reward_max\"]  # 获取当前最大奖励\n",
    "\n",
    "        if max_reward >= self.reward_threshold and not self.saved_checkpoint:\n",
    "            checkpoint_dir = algorithm.save()  # 保存模型\n",
    "            print(f\"模型已保存，路径为：{checkpoint_dir}，最大奖励：{max_reward}\")\n",
    "            self.saved_checkpoint = True  # 确保只保存一次\n",
    "\n",
    "# 更新配置\n",
    "config.update({\n",
    "    \"callbacks\": SaveOnMaxRewardCallback,  # 设置自定义回调\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# 运行训练并保存模型\n",
    "analysis = tune.run(\n",
    "    PPO,\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 50},\n",
    "    local_dir=\"/Users/cyj/Documents/Project/Python/Multi-agent-consensus-algorithm/MARL/ray/tensorboard_logs\",\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=5\n",
    ")\n",
    "\n",
    "\n",
    "# 关闭 Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
