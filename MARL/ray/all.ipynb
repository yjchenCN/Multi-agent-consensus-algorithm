{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "ray.shutdown() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境注册成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 17:09:53,122\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2025-01-03 17:09:53,579\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-03 17:10:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.35        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.0/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_7e859_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-01-03_17-09-51_600290_14960/artifacts/2025-01-03_17-09-53/PPO_2025-01-03_17-09-53/driver_artifacts/PPO_env_7e859_00000_0_2025-01-03_17-09-53/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_7e859_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:246 -- action_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:180 -- observation_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16101, ip=127.0.0.1, actor_id=1d77eae3269d50967b942d4901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x3803a3760>)\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 480, in __init__\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3129, in get_multi_agent_setup\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m     ].observation_space = convert_old_gym_space_to_gymnasium_space(\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/gym.py\", line 67, in convert_old_gym_space_to_gymnasium_space\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m     return gym_space_from_dict(gym_space_to_dict(space))\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 202, in gym_space_to_dict\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m     return _box(space)\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 102, in _box\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m     \"shape\": sp._shape,  # shape is a tuple.\n",
      "\u001b[36m(RolloutWorker pid=16101)\u001b[0m AttributeError: 'Box' object has no attribute '_shape'\n",
      "\u001b[36m(RolloutWorker pid=16098)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:282 -- observation_space_sample() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(RolloutWorker pid=16098)\u001b[0m 2025-01-03 17:10:00,908\tWARNING multi_agent_env.py:209 -- action_space_contains() of <MAEnvironment instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[36m(PPO pid=16093)\u001b[0m 2025-01-03 17:10:00,918\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16098, ip=127.0.0.1, actor_id=19e327d37a786216eafc1a2301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x38097b730>)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m 2025-01-03 17:10:00,918\tERROR actor_manager.py:517 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16099, ip=127.0.0.1, actor_id=b7c4dcdafc176713e02f804001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17f1e3580>)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m 2025-01-03 17:10:00,919\tERROR actor_manager.py:517 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16100, ip=127.0.0.1, actor_id=6c531f987c64277e775a8a6201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x17ebab7c0>)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m 2025-01-03 17:10:00,919\tERROR actor_manager.py:517 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16101, ip=127.0.0.1, actor_id=1d77eae3269d50967b942d4901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x3803a3760>)\n",
      "2025-01-03 17:10:00,925\tERROR tune_controller.py:1332 -- Trial task failed for trial PPO_env_7e859_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=16093, ip=127.0.0.1, actor_id=0caff997712070444c39035101000000, repr=PPO)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16098, ip=127.0.0.1, actor_id=19e327d37a786216eafc1a2301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x38097b730>)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 480, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3129, in get_multi_agent_setup\n",
      "    ].observation_space = convert_old_gym_space_to_gymnasium_space(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/gym.py\", line 67, in convert_old_gym_space_to_gymnasium_space\n",
      "    return gym_space_from_dict(gym_space_to_dict(space))\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 202, in gym_space_to_dict\n",
      "    return _box(space)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 102, in _box\n",
      "    \"shape\": sp._shape,  # shape is a tuple.\n",
      "AttributeError: 'Box' object has no attribute '_shape'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=16093, ip=127.0.0.1, actor_id=0caff997712070444c39035101000000, repr=PPO)\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 533, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 181, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "AttributeError: 'Box' object has no attribute '_shape'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_7e859_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 17:10:00,929\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/Users/cyj/ray_results/PPO_2025-01-03_17-09-53' in 0.0020s.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_env_7e859_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 281\u001b[0m\n\u001b[1;32m    274\u001b[0m config\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: SaveOnMaxRewardCallback,  \u001b[38;5;66;03m# 设置自定义回调\u001b[39;00m\n\u001b[1;32m    276\u001b[0m })\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# 运行训练并保存模型\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_iteration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#local_dir=\"/Users/cyj/Documents/Project/Python/Multi-agent-consensus-algorithm/MARL/ray/tensorboard_logs\",\u001b[39;49;00m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    288\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# 关闭 Ray\u001b[39;00m\n\u001b[1;32m    292\u001b[0m ray\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/tune.py:1042\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1042\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_env_7e859_00000])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=16093)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=16093, ip=127.0.0.1, actor_id=0caff997712070444c39035101000000, repr=PPO)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     self.add_workers(\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     raise result.get()\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     result = ray.get(r)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=16098, ip=127.0.0.1, actor_id=19e327d37a786216eafc1a2301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x38097b730>)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 480, in __init__\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 3129, in get_multi_agent_setup\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     ].observation_space = convert_old_gym_space_to_gymnasium_space(\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/gym.py\", line 67, in convert_old_gym_space_to_gymnasium_space\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     return gym_space_from_dict(gym_space_to_dict(space))\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 202, in gym_space_to_dict\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     return _box(space)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/utils/serialization.py\", line 102, in _box\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     \"shape\": sp._shape,  # shape is a tuple.\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m AttributeError: 'Box' object has no attribute '_shape'\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m \n",
      "\u001b[36m(PPO pid=16093)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m \n",
      "\u001b[36m(PPO pid=16093)\u001b[0m \u001b[36mray::PPO.__init__()\u001b[39m (pid=16093, ip=127.0.0.1, actor_id=0caff997712070444c39035101000000, repr=PPO)\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 533, in __init__\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     super().__init__(\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m   File \"/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 181, in __init__\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[36m(PPO pid=16093)\u001b[0m AttributeError: 'Box' object has no attribute '_shape'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.registry import _global_registry, ENV_CREATOR\n",
    "import os\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "# 定义环境类\n",
    "class MAEnvironment(MultiAgentEnv):\n",
    "    def __init__(self, num_agents=5, num_iterations=200, dt=0.1):\n",
    "        super().__init__()  # 调用父类初始化\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [\"agent_\" + str(i) for i in range(num_agents)]\n",
    "        self.agent_name_mapping = dict(zip(self.agents, list(range(num_agents))))\n",
    "        self._agent_ids = set(self.agents)  # 添加 _agent_ids 属性\n",
    "\n",
    "        # 初始化其他属性\n",
    "        self.num_iterations = num_iterations\n",
    "        self.dt = dt\n",
    "        self.current_iteration = 0\n",
    "\n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "\n",
    "        self.epsilon = 0.005\n",
    "        self.time_to_reach_epsilon = None\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.max_obs_size = self.compute_max_obs_size()\n",
    "    \n",
    "    def compute_max_obs_size(self):\n",
    "        max_neighbors = max(len(agent.neighbors) for agent in self.agent_objs)\n",
    "        return 1 + max_neighbors\n",
    "    \n",
    "    def init_neighbors(self):\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[1])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[0].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[2])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[1].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[3])\n",
    "        self.agent_objs[2].add_neighbor(self.agent_objs[4])\n",
    "        self.agent_objs[3].add_neighbor(self.agent_objs[4])\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        initial_positions = [0.55, 0.4, -0.05, -0.1, -0.7]\n",
    "        self.agent_objs = [self.Agent(pos, i) for i, pos in enumerate(initial_positions)]\n",
    "        self.init_neighbors()\n",
    "        self.current_iteration = 0\n",
    "        self.epsilon_violated = True\n",
    "        self.all_within_epsilon = False\n",
    "        self.total_trigger_count = 0\n",
    "        self.time_to_reach_epsilon_changes = 0\n",
    "        self.time_to_reach_epsilon = None\n",
    "        \n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}  # 返回额外的 per-agent 信息字典\n",
    "        return observations, infos\n",
    "\n",
    "    # 统一的观测空间\n",
    "    def get_observation(self, agent):\n",
    "        agent_index = self.agent_name_mapping[agent]\n",
    "        agent_obj = self.agent_objs[agent_index]\n",
    "        neighbors_positions = [neighbor.position for neighbor in agent_obj.neighbors]\n",
    "        obs = np.array([agent_obj.position] + neighbors_positions, dtype=np.float32)\n",
    "        \n",
    "        # 填充观测到最大观测大小\n",
    "        if len(obs) < self.max_obs_size:\n",
    "            padding = np.zeros(self.max_obs_size - len(obs))\n",
    "            obs = np.concatenate([obs, padding])\n",
    "        \n",
    "        # 不进行裁剪\n",
    "        return obs\n",
    "\n",
    "    # # 不同的观测空间\n",
    "    # def get_observation(self, agent):\n",
    "    #     agent_index = self.agent_name_mapping[agent]\n",
    "    #     agent_obj = self.agent_objs[agent_index]\n",
    "    #     neighbors_positions = [neighbor.position for neighbor in agent_obj.neighbors]\n",
    "    #     obs = np.array([agent_obj.position] + neighbors_positions, dtype=np.float32)\n",
    "    #     return obs\n",
    "\n",
    "    def compute_average_position_difference(self):\n",
    "        total_difference = 0\n",
    "        count = 0\n",
    "        for i, agent_i in enumerate(self.agent_objs):\n",
    "            for j, agent_j in enumerate(self.agent_objs):\n",
    "                if i < j:\n",
    "                    total_difference += abs(agent_i.position - agent_j.position)\n",
    "                    count += 1\n",
    "        if count > 0:\n",
    "            return total_difference / count\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        triggers = np.array([action_dict.get(agent, 0) for agent in self.agents])  # 确保访问安全\n",
    "        trigger_count = np.sum(triggers)\n",
    "        self.total_trigger_count += trigger_count\n",
    "\n",
    "        for i, agent in enumerate(self.agent_objs):\n",
    "            agent.update_position(self.current_iteration, self.dt, triggers[i])\n",
    "\n",
    "        self.all_within_epsilon = all(all(abs(agent.position - neighbor.position) < self.epsilon for neighbor in agent.neighbors) for agent in self.agent_objs)\n",
    "\n",
    "        if self.all_within_epsilon:\n",
    "            if self.epsilon_violated:\n",
    "                self.time_to_reach_epsilon = self.current_iteration\n",
    "                self.epsilon_violated = False\n",
    "                self.time_to_reach_epsilon_changes += 1\n",
    "        else:\n",
    "            self.epsilon_violated = True\n",
    "            self.time_to_reach_epsilon = None\n",
    "        \n",
    "        self.current_iteration += 1\n",
    "        terminated = self.current_iteration >= self.num_iterations\n",
    "\n",
    "        # 根据时间步调整奖励逻辑\n",
    "        early_phase = self.current_iteration <= self.num_iterations * 0.25\n",
    "\n",
    "\n",
    "        rewards = {}\n",
    "        if not terminated:\n",
    "            average_position_difference = self.compute_average_position_difference()\n",
    "            # for agent in self.agents:\n",
    "            #     if self.all_within_epsilon:\n",
    "            #         rewards[agent] = 10 if action_dict.get(agent, 0) == 0 else 0  # 动作为0奖励，1惩罚\n",
    "            #     else:\n",
    "            #         rewards[agent] = - 10 * np.abs(average_position_difference)\n",
    "\n",
    "            for agent in self.agents:\n",
    "                if early_phase:\n",
    "                    # 在前 25% 的时间步，奖励更加注重位置一致性，减少对触发的惩罚\n",
    "                    if self.all_within_epsilon:\n",
    "                        rewards[agent] = 2 if action_dict.get(agent, 0) == 0 else 1\n",
    "                    else:\n",
    "                        rewards[agent] = -1 -10 * np.abs(average_position_difference)\n",
    "                else:\n",
    "                    # 在后 75% 的时间步，更加注重减少触发次数\n",
    "                    if self.all_within_epsilon:\n",
    "                        rewards[agent] = 3 if action_dict.get(agent, 0) == 0 else -0.5\n",
    "                    else:\n",
    "                        rewards[agent] = -1.5 - 10 * np.abs(average_position_difference)\n",
    "        else:\n",
    "            if self.time_to_reach_epsilon is not None:\n",
    "                global_reward = 1250 - self.time_to_reach_epsilon - self.total_trigger_count\n",
    "            else:\n",
    "                global_reward = -10000\n",
    "            for agent in self.agents:\n",
    "                rewards[agent] = global_reward\n",
    "\n",
    "        observations = {agent: self.get_observation(agent) for agent in self.agents}\n",
    "        terminateds = {agent: terminated for agent in self.agents}\n",
    "        terminateds[\"__all__\"] = terminated\n",
    "        truncateds = {agent: False for agent in self.agents}  # 无需提前结束\n",
    "        truncateds[\"__all__\"] = False\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        positions = [agent.position for agent in self.agent_objs]\n",
    "        print(f\"Positions: {positions}\")\n",
    "    \n",
    "    # 统一大小的观测空间\n",
    "    def observation_space(self, agent):\n",
    "        return spaces.Box(low=-100, high=100, shape=(self.max_obs_size,), dtype=np.float32)\n",
    "    \n",
    "    # def observation_space(self, agent):\n",
    "    #     num_neighbors = len(self.agent_objs[self.agent_name_mapping[agent]].neighbors)\n",
    "    #     obs_size = 1 + num_neighbors  # 自身位置 + 邻居数量\n",
    "    #     return spaces.Box(low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float32)\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "    class Agent:\n",
    "        def __init__(self, initial_position, index):\n",
    "            self.position = initial_position\n",
    "            self.index = index\n",
    "            self.neighbors = []\n",
    "            self.last_broadcast_position = self.position\n",
    "            self.trigger_points = []\n",
    "            self.u_i = 0\n",
    "\n",
    "        def add_neighbor(self, neighbor):\n",
    "            if neighbor not in self.neighbors:\n",
    "                self.neighbors.append(neighbor)\n",
    "                neighbor.neighbors.append(self)\n",
    "\n",
    "        def update_position(self, t, dt, trigger):\n",
    "            if trigger == 1 or t == 0:\n",
    "                self.u_i = -sum((self.last_broadcast_position - neighbor.last_broadcast_position) for neighbor in self.neighbors)\n",
    "                self.position += self.u_i * dt\n",
    "                self.last_broadcast_position = self.position\n",
    "                self.trigger_points.append((t, self.position))\n",
    "            else:\n",
    "                self.position += self.u_i * dt\n",
    "\n",
    "# 环境创建函数\n",
    "def env_creator(config):\n",
    "    return MAEnvironment(num_agents=config.get(\"num_agents\", 5))\n",
    "\n",
    "# 注册环境\n",
    "register_env(\"env\", lambda config: MAEnvironment(num_agents=config.get(\"num_agents\", 5)))\n",
    "print(\"环境注册成功\")\n",
    "\n",
    "\n",
    "# 定义共享策略的映射函数\n",
    "def shared_policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    return \"shared_policy\"\n",
    "\n",
    "# 启动 Ray\n",
    "ray.shutdown() \n",
    "ray.init(local_mode=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 配置\n",
    "config = {\n",
    "    \"env\": \"env\",  # 使用注册的环境名\n",
    "    \"env_config\": {\n",
    "        \"num_agents\": 5,  # 传递环境的配置参数\n",
    "    },\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"shared_policy\": (None,  # 使用默认模型\n",
    "                              env_creator({\"num_agents\": 5}).observation_space(\"agent_0\"),  # 观测空间\n",
    "                              env_creator({\"num_agents\": 5}).action_space(\"agent_0\"),  # 动作空间\n",
    "                              {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": shared_policy_mapping_fn,  # 使用共享策略映射\n",
    "    },\n",
    "    \"framework\": \"torch\",  # 使用 \"torch\" 或 \"tf\"\n",
    "    \"num_workers\": 4,  # 使用的工作线程数\n",
    "    \"num_envs_per_worker\": 5,\n",
    "    \"train_batch_size\": 4000, #每次训练时使用的总样本数\n",
    "    \"sgd_minibatch_size\": 256,\n",
    "    \"lr\": 0.0003,\n",
    "    \"num_sgd_iter\": 20,\n",
    "}\n",
    "\n",
    "print(\"开始训练\")\n",
    "\n",
    "class SaveOnMaxRewardCallback(DefaultCallbacks):\n",
    "    def __init__(self, reward_threshold=5000, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reward_threshold = reward_threshold  # 奖励阈值\n",
    "        self.saved_checkpoint = False  # 确保只保存一次模型\n",
    "   \n",
    "    def on_train_result(self, *, algorithm, result, **kwargs):\n",
    "        \"\"\"在每次训练结束时调用，检查最大奖励并保存模型。\"\"\"\n",
    "        max_reward = result[\"episode_reward_max\"]  # 获取当前最大奖励\n",
    "\n",
    "        if max_reward >= self.reward_threshold and not self.saved_checkpoint:\n",
    "            checkpoint_dir = algorithm.save()  # 保存模型\n",
    "            print(f\"模型已保存，路径为：{checkpoint_dir}，最大奖励：{max_reward}\")\n",
    "            self.saved_checkpoint = True  # 确保只保存一次\n",
    "\n",
    "#更新配置\n",
    "config.update({\n",
    "    \"callbacks\": SaveOnMaxRewardCallback,  # 设置自定义回调\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# 运行训练并保存模型\n",
    "analysis = tune.run(\n",
    "    PPO,\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 50},\n",
    "    #local_dir=\"/Users/cyj/Documents/Project/Python/Multi-agent-consensus-algorithm/MARL/ray/tensorboard_logs\",\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=5\n",
    ")\n",
    "\n",
    "\n",
    "# 关闭 Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
