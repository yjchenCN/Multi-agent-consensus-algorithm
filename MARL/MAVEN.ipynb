{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 5, 10]}, size=[4, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 323\u001b[0m\n\u001b[1;32m    321\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    322\u001b[0m maven \u001b[38;5;241m=\u001b[39m MAVEN(env, args, use_cuda)\n\u001b[0;32m--> 323\u001b[0m total_q_values, total_returns \u001b[38;5;241m=\u001b[39m \u001b[43mmaven\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# 可视化总回报变化\u001b[39;00m\n\u001b[1;32m    326\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(total_returns)\n",
      "Cell \u001b[0;32mIn[13], line 229\u001b[0m, in \u001b[0;36mMAVEN.run\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    226\u001b[0m actions_over_time \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_iterations)):\n\u001b[0;32m--> 229\u001b[0m     actions, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     next_obs, rewards, dones, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd((obs, actions, rewards, next_obs, dones))\n",
      "Cell \u001b[0;32mIn[13], line 168\u001b[0m, in \u001b[0;36mMAVEN.choose_action\u001b[0;34m(self, obs, hidden_state, z, epsilon)\u001b[0m\n\u001b[1;32m    166\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents)\n\u001b[1;32m    167\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;28mlist\u001b[39m(obs\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# assuming all agents have the same batch size\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m z_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Expand z to match the batch size\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents):\n\u001b[1;32m    171\u001b[0m     agent_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs[agent], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 5, 10]}, size=[4, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from env.custom_environment import CustomEnvironment\n",
    "\n",
    "# Replay Buffer定义\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Hierarchical Policy定义\n",
    "class HierarchicalPolicy(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(HierarchicalPolicy, self).__init__()\n",
    "        self.fc_1 = nn.Linear(args.state_shape, 128)\n",
    "        self.fc_2 = nn.Linear(128, args.noise_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc_1(state))\n",
    "        q = self.fc_2(x)\n",
    "        prob = F.softmax(q, dim=-1)\n",
    "        return prob\n",
    "\n",
    "# Bootstrapped RNN定义\n",
    "class BootstrappedRNN(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(BootstrappedRNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.fc = nn.Linear(input_shape, args.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)\n",
    "        self.hyper_w = nn.Linear(args.noise_dim + args.n_agents, args.rnn_hidden_dim * args.n_actions)\n",
    "        self.hyper_b = nn.Linear(args.noise_dim + args.n_agents, args.n_actions)\n",
    "\n",
    "    def forward(self, obs, hidden_state, z):\n",
    "        batch_size = obs.size(0)\n",
    "        agent_id = obs[:, -self.args.n_agents:]\n",
    "        z = z.expand(batch_size, -1)\n",
    "        hyper_input = torch.cat([z, agent_id], dim=-1)\n",
    "\n",
    "        x = F.relu(self.fc(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        h = h.view(-1, 1, self.args.rnn_hidden_dim)\n",
    "\n",
    "        hyper_w = self.hyper_w(hyper_input)\n",
    "        hyper_b = self.hyper_b(hyper_input)\n",
    "        hyper_w = hyper_w.view(-1, self.args.rnn_hidden_dim, self.args.n_actions)\n",
    "        hyper_b = hyper_b.view(-1, 1, self.args.n_actions)\n",
    "\n",
    "        q = torch.bmm(h, hyper_w) + hyper_b\n",
    "        q = q.view(-1, self.args.n_actions)\n",
    "        return q, h\n",
    "\n",
    "# Variational Distribution定义\n",
    "class VarDistribution(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VarDistribution, self).__init__()\n",
    "        self.args = args\n",
    "        self.GRU = nn.GRU(args.n_agents * args.n_actions + args.state_shape, 64)\n",
    "        self.fc_1 = nn.Linear(64, 32)\n",
    "        self.fc_2 = nn.Linear(32, args.noise_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, h = self.GRU(inputs)\n",
    "        x = F.relu(self.fc_1(h.squeeze(0)))\n",
    "        x = self.fc_2(x)\n",
    "        output = F.softmax(x, dim=-1)\n",
    "        return output\n",
    "\n",
    "# QMixNet定义\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.args = args\n",
    "        if args.two_hyper_layers:\n",
    "            self.hyper_w1 = nn.Sequential(nn.Linear(args.state_shape, args.hyper_hidden_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(args.hyper_hidden_dim, args.n_agents * args.qmix_hidden_dim))\n",
    "            self.hyper_w2 = nn.Sequential(nn.Linear(args.state_shape, args.hyper_hidden_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(args.hyper_hidden_dim, args.qmix_hidden_dim))\n",
    "        else:\n",
    "            self.hyper_w1 = nn.Linear(args.state_shape, args.n_agents * args.qmix_hidden_dim)\n",
    "            self.hyper_w2 = nn.Linear(args.state_shape, args.qmix_hidden_dim * 1)\n",
    "        self.hyper_b1 = nn.Linear(args.state_shape, args.qmix_hidden_dim)\n",
    "        self.hyper_b2 = nn.Sequential(nn.Linear(args.state_shape, args.qmix_hidden_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(args.qmix_hidden_dim, 1))\n",
    "\n",
    "    def forward(self, q_values, states):\n",
    "        episode_num = q_values.size(0)\n",
    "        q_values = q_values.view(episode_num, 1, self.args.n_agents)\n",
    "        states = states.view(episode_num, -1)\n",
    "        w1 = torch.abs(self.hyper_w1(states))\n",
    "        b1 = self.hyper_b1(states)\n",
    "        w1 = w1.view(episode_num, self.args.n_agents, self.args.qmix_hidden_dim)\n",
    "        b1 = b1.view(episode_num, 1, self.args.qmix_hidden_dim)\n",
    "        hidden = F.elu(torch.bmm(q_values, w1) + b1)\n",
    "        w2 = torch.abs(self.hyper_w2(states))\n",
    "        b2 = self.hyper_b2(states)\n",
    "        w2 = w2.view(episode_num, self.args.qmix_hidden_dim, 1)\n",
    "        b2 = b2.view(episode_num, 1, 1)\n",
    "        q_total = torch.bmm(hidden, w2) + b2\n",
    "        q_total = q_total.view(episode_num, -1, 1)\n",
    "        return q_total\n",
    "\n",
    "class MAVEN:\n",
    "    def __init__(self, env, args, use_cuda=False):\n",
    "        self.env = env\n",
    "        self.args = args\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "        self.replay_buffer = ReplayBuffer(args.buffer_size)\n",
    "        self.z_policy = HierarchicalPolicy(args).to(self.device)\n",
    "        self.eval_rnn = BootstrappedRNN(args.obs_shape, args).to(self.device)\n",
    "        self.target_rnn = BootstrappedRNN(args.obs_shape, args).to(self.device)\n",
    "        self.eval_qmix_net = QMixNet(args).to(self.device)\n",
    "        self.target_qmix_net = QMixNet(args).to(self.device)\n",
    "        self.mi_net = VarDistribution(args).to(self.device)\n",
    "        self.optimizer = Adam(list(self.z_policy.parameters()) +\n",
    "                              list(self.eval_qmix_net.parameters()) +\n",
    "                              list(self.eval_rnn.parameters()) +\n",
    "                              list(self.mi_net.parameters()), lr=args.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.eval_hidden = None\n",
    "        self.target_hidden = None\n",
    "        self.model_dir = args.model_dir + '/' + args.alg + '/' + args.map\n",
    "        if self.args.load_model:\n",
    "            if os.path.exists(self.model_dir + '/rnn_net_params.pkl'):\n",
    "                path_z_policy = self.model_dir + '/z_policy_params.pkl'\n",
    "                path_rnn = self.model_dir + '/rnn_net_params.pkl'\n",
    "                path_qmix = self.model_dir + '/qmix_net_params.pkl'\n",
    "                path_mi = self.model_dir + '/mi_net_params.pkl'\n",
    "                map_location = 'cuda:0' if self.args.cuda else 'cpu'\n",
    "                self.z_policy.load_state_dict(torch.load(path_z_policy, map_location=map_location))\n",
    "                self.eval_rnn.load_state_dict(torch.load(path_rnn, map_location=map_location))\n",
    "                self.eval_qmix_net.load_state_dict(torch.load(path_qmix, map_location=map_location))\n",
    "                self.mi_net.load_state_dict(torch.load(path_mi, map_location=map_location))\n",
    "                print('Successfully load the model: {}, {}, {} and {}'.format(path_z_policy, path_rnn, path_qmix, path_mi))\n",
    "            else:\n",
    "                raise Exception(\"No model!\")\n",
    "        self.target_rnn.load_state_dict(self.eval_rnn.state_dict())\n",
    "        self.target_qmix_net.load_state_dict(self.eval_qmix_net.state_dict())\n",
    "\n",
    "    def choose_action(self, obs, hidden_state, z, epsilon=0.01):\n",
    "        actions = {}\n",
    "        new_hidden_state = {}\n",
    "        num_agents = len(self.env.agents)\n",
    "        batch_size = obs[list(obs.keys())[0]].shape[0]  # assuming all agents have the same batch size\n",
    "        z_expanded = z.expand(batch_size, -1)  # Expand z to match the batch size\n",
    "\n",
    "        for i, agent in enumerate(self.env.agents):\n",
    "            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "            z_agent = z_expanded  # Use expanded z for the current agent\n",
    "\n",
    "            q_values, new_agent_hidden_state = self.eval_rnn(agent_obs, agent_hidden_state, z_agent)\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(range(self.args.n_actions))\n",
    "            else:\n",
    "                action = q_values.argmax(dim=-1).item()\n",
    "            actions[agent] = action\n",
    "            new_hidden_state[agent] = new_agent_hidden_state.detach().cpu().numpy()\n",
    "        return actions, new_hidden_state\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.args.minimal_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.args.batch_size)\n",
    "        states = {agent: torch.tensor([s[agent] for s in states], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        actions = {agent: torch.tensor([a[agent] for a in actions], dtype=torch.int64).to(self.device) for agent in self.env.agents}\n",
    "        rewards = {agent: torch.tensor([r[agent] for r in rewards], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_states = {agent: torch.tensor([ns[agent] for ns in next_states], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        dones = {agent: torch.tensor([d[agent] for d in dones], dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        hidden_state = {agent: torch.zeros((self.args.batch_size, self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "        next_hidden_state = {agent: torch.zeros((self.args.batch_size, self.args.rnn_hidden_dim), dtype=torch.float32).to(self.device) for agent in self.env.agents}\n",
    "\n",
    "        q_values = {agent: self.eval_rnn(states[agent], hidden_state[agent], z)[0].gather(1, actions[agent].unsqueeze(-1)).squeeze(-1) for agent in self.env.agents}\n",
    "        next_q_values = {agent: self.target_rnn(next_states[agent], next_hidden_state[agent], z)[0].max(dim=-1)[0] for agent in self.env.agents}\n",
    "\n",
    "        targets = {agent: rewards[agent] + self.args.gamma * next_q_values[agent] * (1 - dones[agent]) for agent in self.env.agents}\n",
    "\n",
    "        q_values_tensor = torch.stack(list(q_values.values()), dim=1)\n",
    "        targets_tensor = torch.stack(list(targets.values()), dim=1)\n",
    "        states_tensor = torch.cat([states[agent].view(self.args.batch_size, -1) for agent in self.env.agents], dim=1)\n",
    "\n",
    "        q_total = self.eval_qmix_net(q_values_tensor, states_tensor)\n",
    "        targets_total = self.target_qmix_net(targets_tensor, states_tensor)\n",
    "\n",
    "        loss = self.loss_fn(q_total, targets_total.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self, num_episodes):\n",
    "        total_q_values = []\n",
    "        total_returns = []\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "                for episode in range(int(num_episodes / 10)):\n",
    "                    obs = self.env.reset()\n",
    "                    hidden_state = {agent: np.zeros(self.args.rnn_hidden_dim) for agent in self.env.agents}\n",
    "                    state_tensor = torch.tensor([obs[agent] for agent in self.env.agents], dtype=torch.float32).to(self.device)\n",
    "                    z = self.z_policy(state_tensor).unsqueeze(0)\n",
    "                    episode_q_values = []\n",
    "                    episode_return = 0\n",
    "                    actions_over_time = []\n",
    "\n",
    "                    for t in range(int(self.args.num_iterations)):\n",
    "                        actions, hidden_state = self.choose_action(obs, hidden_state, z)\n",
    "                        next_obs, rewards, dones, _ = self.env.step(actions)\n",
    "                        self.replay_buffer.add((obs, actions, rewards, next_obs, dones))\n",
    "                        obs = next_obs\n",
    "\n",
    "                        actions_over_time.append(actions)\n",
    "\n",
    "                        q_values = []\n",
    "                        for agent in self.env.agents:\n",
    "                            agent_obs = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_hidden_state = torch.tensor(hidden_state[agent], dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                            agent_q_values, _ = self.eval_rnn(agent_obs, agent_hidden_state, z)\n",
    "                            q_values.append(agent_q_values.max().item())\n",
    "                        episode_q_values.append(np.mean(q_values))\n",
    "\n",
    "                        episode_return += sum(rewards.values())\n",
    "\n",
    "                        self.train()\n",
    "\n",
    "                        if all(dones.values()):\n",
    "                            break\n",
    "\n",
    "                    total_q_values.append(np.mean(episode_q_values))\n",
    "                    total_returns.append(episode_return)\n",
    "\n",
    "                    if episode_return > 10000:\n",
    "                        print(f\"Episode {episode + 1} reached a total return of {episode_return}\")\n",
    "                        for t, actions in enumerate(actions_over_time):\n",
    "                            actions_list = [actions[agent] for agent in self.env.agents]\n",
    "                            print(actions_list)\n",
    "                        print(\"done\")\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix({'episode': '%d' % (num_episodes / 10 * i + episode + 1), 'returns': '%.3f' % np.mean(total_returns[-10:])})\n",
    "                    pbar.update(1)\n",
    "\n",
    "        return total_q_values, total_returns\n",
    "\n",
    "    def run_one_iteration(self):\n",
    "        obs = self.env.reset()\n",
    "        hidden_state = {agent: np.zeros(self.args.rnn_hidden_dim) for agent in self.env.agents}\n",
    "        z = self.z_policy(torch.tensor([obs[agent] for agent in self.env.agents], dtype=torch.float32).to(self.device))\n",
    "\n",
    "        actions_over_time = []\n",
    "\n",
    "        for t in range(self.args.num_iterations):\n",
    "            actions, hidden_state = self.choose_action(obs, hidden_state, z)\n",
    "            next_obs, rewards, dones, _ = self.env.step(actions)\n",
    "            actions_over_time.append(actions)\n",
    "            if all(dones.values()):\n",
    "                break\n",
    "            obs = next_obs\n",
    "\n",
    "        for t, actions in enumerate(actions_over_time):\n",
    "            print(f\"Time step {t}:\")\n",
    "            for agent, action in actions.items():\n",
    "                print(f\"  Agent {agent}: Action {action}\")\n",
    "\n",
    "        return actions, next_obs, rewards, dones\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def  __init__(self, env):\n",
    "        self.state_shape = env.max_obs_size\n",
    "        self.obs_shape = env.max_obs_size\n",
    "        self.n_agents = 5\n",
    "        self.n_actions = 2\n",
    "        self.qmix_hidden_dim = 32\n",
    "        self.hyper_hidden_dim = 64\n",
    "        self.rnn_hidden_dim = 64\n",
    "        self.two_hyper_layers = False\n",
    "        self.buffer_size = 100000\n",
    "        self.minimal_size = 65\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.95\n",
    "        self.lr = 1e-3\n",
    "        self.num_iterations = env.num_iterations\n",
    "        self.num_episodes = 1000\n",
    "        self.noise_dim = 10\n",
    "        self.entropy_coefficient = 0.01\n",
    "        self.lambda_mi = 1.0\n",
    "        self.lambda_ql = 1.0\n",
    "        self.grad_norm_clip = 10\n",
    "        self.model_dir = ''\n",
    "        self.alg = ''\n",
    "        self.map = ''\n",
    "        self.load_model = False\n",
    "        self.cuda = False\n",
    "\n",
    "env = CustomEnvironment()\n",
    "torch.manual_seed(0)\n",
    "args = Args(env)\n",
    "\n",
    "use_cuda = False\n",
    "maven = MAVEN(env, args, use_cuda)\n",
    "total_q_values, total_returns = maven.run(args.num_episodes)\n",
    "\n",
    "# 可视化总回报变化\n",
    "plt.plot(total_returns)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Return')\n",
    "plt.title('Total Return over episodes')\n",
    "plt.show()\n",
    "\n",
    "# 可视化Q值变化\n",
    "plt.plot(total_q_values)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Q value')\n",
    "plt.title('Total Q value over episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
