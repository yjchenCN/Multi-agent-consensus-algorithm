{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1, 5]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 170\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     env \u001b[38;5;241m=\u001b[39m CustomEnvironment()\n\u001b[0;32m--> 170\u001b[0m     returns, q_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_qmix\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     plot_results(returns, q_values)\n",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m, in \u001b[0;36mtrain_qmix\u001b[0;34m(env, n_episodes, batch_size, gamma, lr)\u001b[0m\n\u001b[1;32m    139\u001b[0m agent_qs_next \u001b[38;5;241m=\u001b[39m agent_net(next_obs)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    140\u001b[0m target_q_tot \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m agent_qs_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done)\n\u001b[0;32m--> 142\u001b[0m q_tot \u001b[38;5;241m=\u001b[39m \u001b[43mqmix_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_qs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_tot, target_q_tot)\n\u001b[1;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36mQMixNet.forward\u001b[0;34m(self, agent_qs, states)\u001b[0m\n\u001b[1;32m     52\u001b[0m w1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_w_1(states))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m     53\u001b[0m b1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_b_1(states)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m---> 54\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mbmm(\u001b[43magent_qs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_agents\u001b[49m\u001b[43m)\u001b[49m, w1) \u001b[38;5;241m+\u001b[39m b1)\n\u001b[1;32m     56\u001b[0m w_final \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_w_final(states))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m b_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_b_final(states)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1, 5]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pettingzoo import ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from env.custom_environment import CustomEnvironment\n",
    "\n",
    "# 定义Agent网络\n",
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义QMIX网络\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape, mixing_embed_dim):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.state_shape = state_shape\n",
    "        self.embed_dim = mixing_embed_dim\n",
    "        \n",
    "        self.hyper_w_1 = nn.Sequential(nn.Linear(state_shape, self.embed_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(self.embed_dim, n_agents * self.embed_dim))\n",
    "        self.hyper_w_final = nn.Sequential(nn.Linear(state_shape, self.embed_dim),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(self.embed_dim, self.embed_dim))\n",
    "        \n",
    "        self.hyper_b_1 = nn.Linear(state_shape, self.embed_dim)\n",
    "        self.hyper_b_final = nn.Sequential(nn.Linear(state_shape, self.embed_dim),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(self.embed_dim, 1))\n",
    "        \n",
    "        self.V = nn.Sequential(nn.Linear(state_shape, self.embed_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(self.embed_dim, 1))\n",
    "        \n",
    "    def forward(self, agent_qs, states):\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_shape)\n",
    "        \n",
    "        w1 = torch.abs(self.hyper_w_1(states)).view(-1, self.n_agents, self.embed_dim)\n",
    "        b1 = self.hyper_b_1(states).view(-1, 1, self.embed_dim)\n",
    "        hidden = torch.relu(torch.bmm(agent_qs.view(-1, 1, self.n_agents), w1) + b1)\n",
    "        \n",
    "        w_final = torch.abs(self.hyper_w_final(states)).view(-1, self.embed_dim, 1)\n",
    "        b_final = self.hyper_b_final(states).view(-1, 1, 1)\n",
    "        \n",
    "        y = torch.bmm(hidden, w_final) + b_final\n",
    "        q_tot = y.view(bs, -1, 1).squeeze(-1)\n",
    "        \n",
    "        q_tot = q_tot + self.V(states).view(bs, -1)\n",
    "        \n",
    "        return q_tot\n",
    "\n",
    "# 经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, obs, action, reward, next_state, next_obs, done):\n",
    "        self.buffer.append((state, obs, action, reward, next_state, next_obs, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, obs, action, reward, next_state, next_obs, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(obs), np.array(action), np.array(reward), np.array(next_state), np.array(next_obs), np.array(done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# QMIX 训练代码\n",
    "def train_qmix(env, n_episodes=1000, batch_size=32, gamma=0.99, lr=0.0005):\n",
    "    n_agents = len(env.agents)\n",
    "    obs_shape = env.observation_space(env.agents[0]).shape[0]\n",
    "    state_shape = env.get_state().shape[0]\n",
    "    n_actions = env.action_space(env.agents[0]).n\n",
    "    \n",
    "    agent_net = AgentNetwork(obs_shape, n_actions)\n",
    "    qmix_net = QMixNet(n_agents, state_shape, 32)\n",
    "    \n",
    "    optimizer = optim.Adam(list(agent_net.parameters()) + list(qmix_net.parameters()), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(5000)\n",
    "    \n",
    "    returns = []\n",
    "    q_values = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = env.get_state()\n",
    "        episode_reward = 0\n",
    "        episode_q_values = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for agent in env.agents:\n",
    "                obs_tensor = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0)\n",
    "                q_values_agent = agent_net(obs_tensor)\n",
    "                action = q_values_agent.argmax().item()\n",
    "                actions[agent] = action\n",
    "                episode_q_values.append(q_values_agent.max().item())\n",
    "            \n",
    "            next_obs, rewards, dones, infos = env.step(actions)\n",
    "            next_state = env.get_state()\n",
    "            \n",
    "            for agent in env.agents:\n",
    "                replay_buffer.push(state, obs[agent], actions[agent], rewards[agent], next_state, next_obs[agent], dones[agent])\n",
    "            \n",
    "            obs = next_obs\n",
    "            state = next_state\n",
    "            episode_reward += sum(rewards.values())\n",
    "            done = all(dones.values())\n",
    "        \n",
    "        returns.append(episode_reward)\n",
    "        q_values.append(np.mean(episode_q_values))\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            state, obs, action, reward, next_state, next_obs, done = replay_buffer.sample(batch_size)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            action = torch.tensor(action, dtype=torch.long)\n",
    "            reward = torch.tensor(reward, dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32)\n",
    "            done = torch.tensor(done, dtype=torch.float32)\n",
    "            \n",
    "            agent_qs = agent_net(obs).gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            agent_qs_next = agent_net(next_obs).max(1)[0]\n",
    "            target_q_tot = reward + gamma * agent_qs_next * (1 - done)\n",
    "            \n",
    "            q_tot = qmix_net(agent_qs, state)\n",
    "            loss = nn.MSELoss()(q_tot, target_q_tot)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return returns, q_values\n",
    "\n",
    "# 绘制结果\n",
    "def plot_results(returns, q_values):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    ax[0].plot(returns)\n",
    "    ax[0].set_title(\"Total Returns Over Time\")\n",
    "    ax[0].set_xlabel(\"Episodes\")\n",
    "    ax[0].set_ylabel(\"Total Return\")\n",
    "    \n",
    "    ax[1].plot(q_values)\n",
    "    ax[1].set_title(\"Average Q-Value Over Time\")\n",
    "    ax[1].set_xlabel(\"Episodes\")\n",
    "    ax[1].set_ylabel(\"Average Q-Value\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    env = CustomEnvironment()\n",
    "    returns, q_values = train_qmix(env)\n",
    "    plot_results(returns, q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
