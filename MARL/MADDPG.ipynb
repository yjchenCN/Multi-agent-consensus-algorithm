{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# 使用自定义环境训练MADDPG\u001b[39;00m\n\u001b[1;32m    155\u001b[0m env \u001b[38;5;241m=\u001b[39m CustomEnvironment()\n\u001b[0;32m--> 156\u001b[0m trained_agents \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_maddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 122\u001b[0m, in \u001b[0;36mtrain_maddpg\u001b[0;34m(env, num_episodes, batch_size, memory_capacity, max_steps)\u001b[0m\n\u001b[1;32m    119\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m--> 122\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent: agents[i]\u001b[38;5;241m.\u001b[39mselect_action(observations[agent]) \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(env\u001b[38;5;241m.\u001b[39magents)}\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(actions)\n\u001b[1;32m    125\u001b[0m     next_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[0;32mIn[8], line 122\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m--> 122\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent: \u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(env\u001b[38;5;241m.\u001b[39magents)}\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(actions)\n\u001b[1;32m    125\u001b[0m     next_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[0;32mIn[8], line 61\u001b[0m, in \u001b[0;36mDDPGAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     59\u001b[0m action_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 选择动作0或1\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action_prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from env.custom_environment import CustomEnvironment\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, tau=0.01):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "        self.critic = Critic(obs_dim + action_dim)\n",
    "        self.target_actor = Actor(obs_dim, action_dim)\n",
    "        self.target_critic = Critic(obs_dim + action_dim)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.update_targets(tau=1.0)\n",
    "\n",
    "    def update_targets(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_prob = self.actor(state).squeeze(0).detach().numpy()\n",
    "        # 选择动作0或1\n",
    "        action = 1 if action_prob >= 0 else 0\n",
    "        print(action)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def train(self, memory, batch_size):\n",
    "        if len(memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        next_actions = self.target_actor(next_states)\n",
    "        target_q_values = self.target_critic(torch.cat([next_states, next_actions], 1))\n",
    "        q_targets = rewards + self.gamma * target_q_values * (1 - dones)\n",
    "\n",
    "        q_values = self.critic(torch.cat([states, actions], 1))\n",
    "        critic_loss = torch.mean((q_values - q_targets.detach()) ** 2)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = -self.critic(torch.cat([states, self.actor(states)], 1)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.update_targets()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train_maddpg(env, num_episodes=1000, batch_size=64, memory_capacity=100000, max_steps=200):\n",
    "    memory = ReplayBuffer(memory_capacity)\n",
    "    agents = [DDPGAgent(env.observation_space(agent).shape[0], env.action_space(agent).n) for agent in env.agents]\n",
    "\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observations = env.reset()\n",
    "        episode_rewards = {agent: 0 for agent in env.agents}\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            actions = {agent: agents[i].select_action(observations[agent]) for i, agent in enumerate(env.agents)}\n",
    "            print(actions)\n",
    "\n",
    "            next_observations, rewards, dones, infos = env.step(actions)\n",
    "            for i, agent in enumerate(env.agents):\n",
    "                memory.push(observations[agent], actions[agent], rewards[agent], next_observations[agent], dones[agent])\n",
    "\n",
    "            for agent in agents:\n",
    "                agent.train(memory, batch_size)\n",
    "\n",
    "            observations = next_observations\n",
    "\n",
    "            for agent in env.agents:\n",
    "                episode_rewards[agent] += rewards[agent]\n",
    "\n",
    "            if all(dones.values()):\n",
    "                break\n",
    "\n",
    "        total_episode_reward = sum(episode_rewards.values())\n",
    "        total_rewards.append(total_episode_reward)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f'Episode {episode + 1}, Total Reward: {total_episode_reward}')\n",
    "\n",
    "    plt.plot(total_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward vs. Episode')\n",
    "    plt.show()\n",
    "\n",
    "    return agents\n",
    "\n",
    "# 使用自定义环境训练MADDPG\n",
    "env = CustomEnvironment()\n",
    "trained_agents = train_maddpg(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "详细阅读我的代码，帮我写一个能训练我环境的多智能体强化学习算法，具体是什么算法你自己决定。你一定要写一个适配我环境的算法，并且要能运行，每十步输出当前的总回报值，并在最后绘制出奖励的变化曲线。\n",
    "关于我的环境我有几点要说明：我的环境的总体意思是有五个智能体，他们能选取的动作只有0或1，0为不触发1为触发，最终要在200步内达成实现一致性并且尽可能少触发。算法就是要训练出这两百步内每一步中五个智能体的动作选择，来让奖励最大化。要注意每个智能体能选择的动作只有0或1，然后每一步的actions为五个智能体动作选择的拼接矩阵，类似于[1，0，0，0，1],要以这种格式与环境交互。其他的一些点你也要与我的环境相匹配。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
