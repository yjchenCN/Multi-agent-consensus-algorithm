{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39magents)\n\u001b[1;32m    156\u001b[0m multi_agent_ddpg \u001b[38;5;241m=\u001b[39m MultiAgentDDPG(env, num_agents, state_size, action_size)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mmulti_agent_ddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 104\u001b[0m, in \u001b[0;36mMultiAgentDDPG.train\u001b[0;34m(self, num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m    101\u001b[0m episode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([rewards[agent] \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magents])\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_networks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dones\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m, in \u001b[0;36mMultiAgentDDPG.update_networks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents):\n\u001b[0;32m--> 117\u001b[0m     state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([s[i] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[1;32m    118\u001b[0m     action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[a[i]] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m actions])\n\u001b[1;32m    119\u001b[0m     reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rewards])\n",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents):\n\u001b[0;32m--> 117\u001b[0m     state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[1;32m    118\u001b[0m     action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[a[i]] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m actions])\n\u001b[1;32m    119\u001b[0m     reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rewards])\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from env.custom_environment import CustomEnvironment\n",
    "\n",
    "# 定义Q网络\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 定义策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# 定义经验回放\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 多智能体DDPG训练\n",
    "class MultiAgentDDPG:\n",
    "    def __init__(self, env, num_agents, state_size, action_size, buffer_size=10000, batch_size=64, gamma=0.99, lr=0.001, tau=0.01):\n",
    "        self.env = env\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.q_networks = [QNetwork(state_size + action_size * num_agents, 1) for _ in range(num_agents)]\n",
    "        self.target_q_networks = [QNetwork(state_size + action_size * num_agents, 1) for _ in range(num_agents)]\n",
    "        self.policy_networks = [PolicyNetwork(state_size, action_size) for _ in range(num_agents)]\n",
    "        self.target_policy_networks = [PolicyNetwork(state_size, action_size) for _ in range(num_agents)]\n",
    "        \n",
    "        self.q_optimizers = [optim.Adam(q_net.parameters(), lr=self.lr) for q_net in self.q_networks]\n",
    "        self.policy_optimizers = [optim.Adam(policy_net.parameters(), lr=self.lr) for policy_net in self.policy_networks]\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        for target_q_net, q_net in zip(self.target_q_networks, self.q_networks):\n",
    "            target_q_net.load_state_dict(q_net.state_dict())\n",
    "        \n",
    "        for target_policy_net, policy_net in zip(self.target_policy_networks, self.policy_networks):\n",
    "            target_policy_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    def select_actions(self, states, epsilon):\n",
    "        actions = []\n",
    "        for i in range(self.num_agents):\n",
    "            state = torch.FloatTensor(states[i]).unsqueeze(0)\n",
    "            action = self.policy_networks[i](state).detach().numpy()[0]\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, self.action_size - 1)\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def train(self, num_episodes, max_steps, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
    "        epsilon = epsilon_start\n",
    "        for episode in range(num_episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_rewards = np.zeros(self.num_agents)\n",
    "            for step in range(max_steps):\n",
    "                actions = self.select_actions([states[agent] for agent in self.env.agents], epsilon)\n",
    "                next_states, rewards, dones, _ = self.env.step({agent: action for agent, action in zip(self.env.agents, actions)})\n",
    "                \n",
    "                self.replay_buffer.add((states, actions, rewards, next_states, dones))\n",
    "                states = next_states\n",
    "                episode_rewards += np.array([rewards[agent] for agent in self.env.agents])\n",
    "                \n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    self.update_networks()\n",
    "                \n",
    "                if all(dones.values()):\n",
    "                    break\n",
    "            \n",
    "            epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {np.sum(episode_rewards)}\")\n",
    "    \n",
    "    def update_networks(self):\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        for i in range(self.num_agents):\n",
    "            state_batch = torch.FloatTensor([s[i] for s in states])\n",
    "            action_batch = torch.FloatTensor([[a[i]] for a in actions])\n",
    "            reward_batch = torch.FloatTensor([r[i] for r in rewards])\n",
    "            next_state_batch = torch.FloatTensor([ns[i] for ns in next_states])\n",
    "            done_batch = torch.FloatTensor([d[i] for d in dones])\n",
    "            \n",
    "            all_states = torch.FloatTensor([np.concatenate(s) for s in states])\n",
    "            all_actions = torch.FloatTensor([np.concatenate(a) for a in actions])\n",
    "            all_next_states = torch.FloatTensor([np.concatenate(ns) for ns in next_states])\n",
    "            \n",
    "            current_q_values = self.q_networks[i](torch.cat([state_batch, all_actions.view(self.batch_size, -1)], 1))\n",
    "            next_actions = [self.target_policy_networks[j](next_state_batch) for j in range(self.num_agents)]\n",
    "            target_q_values = self.target_q_networks[i](torch.cat([next_state_batch, torch.cat(next_actions, 1)], 1)).detach()\n",
    "            target_q_values = reward_batch + (self.gamma * target_q_values * (1 - done_batch))\n",
    "            \n",
    "            q_loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "            self.q_optimizers[i].zero_grad()\n",
    "            q_loss.backward()\n",
    "            self.q_optimizers[i].step()\n",
    "            \n",
    "            all_current_actions = [self.policy_networks[j](state_batch) if j == i else self.policy_networks[j](torch.FloatTensor([s[j] for s in states])) for j in range(self.num_agents)]\n",
    "            policy_loss = -self.q_networks[i](torch.cat([state_batch, torch.cat(all_current_actions, 1)], 1)).mean()\n",
    "            self.policy_optimizers[i].zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizers[i].step()\n",
    "            \n",
    "            self.soft_update(self.q_networks[i], self.target_q_networks[i])\n",
    "            self.soft_update(self.policy_networks[i], self.target_policy_networks[i])\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "# 使用自定义环境进行训练\n",
    "env = CustomEnvironment()\n",
    "state_size = env.max_obs_size\n",
    "action_size = env.action_space(env.agents[0]).n\n",
    "num_agents = len(env.agents)\n",
    "\n",
    "multi_agent_ddpg = MultiAgentDDPG(env, num_agents, state_size, action_size)\n",
    "multi_agent_ddpg.train(num_episodes=1000, max_steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
