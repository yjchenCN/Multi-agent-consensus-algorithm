{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_consensus_d_f_tensorboard/PPO_8\n",
      "---------------------------------------\n",
      "| custom/                  |          |\n",
      "|    time_to_reach_epsilon | 70       |\n",
      "|    total_trigger_count   | 487      |\n",
      "| rollout/                 |          |\n",
      "|    ep_len_mean           | 200      |\n",
      "|    ep_rew_mean           | 2.66e+03 |\n",
      "| time/                    |          |\n",
      "|    fps                   | 6376     |\n",
      "|    iterations            | 1        |\n",
      "|    time_elapsed          | 0        |\n",
      "|    total_timesteps       | 2048     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m trigger_count_callback \u001b[38;5;241m=\u001b[39m TriggerCountCallback()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 开始训练模型，设置训练的时间步数\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigger_count_callback\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 根据需要更改时间步数\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 保存训练好的模型\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_consensus_d_f\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:278\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/adam.py:292\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Respect when the user inputs False/True for foreach or fused. We only want to change\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# the default when neither have been user-specified. Note that we default to foreach\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# and pass False to use_fused. This is not a mistake--we want to give the fused impl\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# bake-in time before making it the default, even if it is typically faster.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     _, foreach \u001b[38;5;241m=\u001b[39m \u001b[43m_default_to_fused_or_foreach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py:115\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m fused_supported_devices \u001b[38;5;241m=\u001b[39m \u001b[43m_get_fused_kernels_supported_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_foreach_utils.py:14\u001b[0m, in \u001b[0;36m_get_fused_kernels_supported_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_fused_kernels_supported_devices\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the device type list that supports fused kernels in optimizer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_privateuse1_backend_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import tensorflow as tf\n",
    "from env1 import Consensus_D_F  # 从env1.py导入自定义环境\n",
    "\n",
    "# 确保环境符合Gym的规范\n",
    "env = Consensus_D_F(num_agents=5, num_iterations=200, dt=0.1)\n",
    "# 使用稳定性检查器确保环境符合要求\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)\n",
    "\n",
    "# 创建环境实例\n",
    "env = Consensus_D_F()\n",
    "\n",
    "# 定义PPO模型，您可以根据需求调整超参数\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # 使用多层感知器的策略网络\n",
    "    env,            # 自定义环境\n",
    "    verbose=1,      # 输出训练信息\n",
    "    tensorboard_log=\"./ppo_consensus_d_f_tensorboard/\"  # 日志存储路径（可选）\n",
    ")\n",
    "\n",
    "class TriggerCountCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TriggerCountCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        # 检查是否是 episode 的最后一步\n",
    "        if self.locals[\"dones\"][0]:  # 当 episode 结束时\n",
    "            total_trigger_count = self.training_env.get_attr(\"total\")[0]\n",
    "            time_to_reach_epsilon = self.training_env.get_attr(\"time\")[0]\n",
    "            \n",
    "            # 记录到 TensorBoard\n",
    "            self.logger.record(\"custom/total_trigger_count\", total_trigger_count)\n",
    "            if time_to_reach_epsilon is not None:\n",
    "                self.logger.record(\"custom/time_to_reach_epsilon\", time_to_reach_epsilon)\n",
    "        return True\n",
    "\n",
    "trigger_count_callback = TriggerCountCallback()\n",
    "\n",
    "# 开始训练模型，设置训练的时间步数\n",
    "model.learn(total_timesteps=5000000, callback=trigger_count_callback)  # 根据需要更改时间步数\n",
    "\n",
    "# 保存训练好的模型\n",
    "model.save(\"ppo_consensus_d_f\")\n",
    "\n",
    "# # 加载模型并测试\n",
    "# model = PPO.load(\"ppo_consensus_d_f\", env=env)\n",
    "\n",
    "# # 测试模型，运行一个episode\n",
    "# # 测试模型，运行一个 episode\n",
    "# obs, _ = env.reset()  # 只获取 obs，忽略 info\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, _, info = env.step(action)  # 忽略 `truncated`，只解包前四个值\n",
    "#     #env.render()  # 若环境中有 render 函数，可以进行渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_consensus_d_f_tensorboard1/PPO_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| custom/                  |          |\n",
      "|    time_to_reach_epsilon | 38       |\n",
      "|    total_trigger_count   | 510      |\n",
      "| rollout/                 |          |\n",
      "|    ep_len_mean           | 200      |\n",
      "|    ep_rew_mean           | 2.49e+03 |\n",
      "| time/                    |          |\n",
      "|    fps                   | 4968     |\n",
      "|    iterations            | 1        |\n",
      "|    time_elapsed          | 0        |\n",
      "|    total_timesteps       | 2048     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| custom/                  |             |\n",
      "|    time_to_reach_epsilon | 41          |\n",
      "|    total_trigger_count   | 496         |\n",
      "| rollout/                 |             |\n",
      "|    ep_len_mean           | 200         |\n",
      "|    ep_rew_mean           | 2.5e+03     |\n",
      "| time/                    |             |\n",
      "|    fps                   | 3551        |\n",
      "|    iterations            | 2           |\n",
      "|    time_elapsed          | 1           |\n",
      "|    total_timesteps       | 4096        |\n",
      "| train/                   |             |\n",
      "|    approx_kl             | 0.000347221 |\n",
      "|    clip_fraction         | 0.000146    |\n",
      "|    clip_range            | 0.2         |\n",
      "|    entropy_loss          | -3.47       |\n",
      "|    explained_variance    | 2.96e-05    |\n",
      "|    learning_rate         | 0.0003      |\n",
      "|    loss                  | 2.6e+04     |\n",
      "|    n_updates             | 10          |\n",
      "|    policy_gradient_loss  | -0.00239    |\n",
      "|    value_loss            | 5.62e+04    |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 42            |\n",
      "|    total_trigger_count   | 516           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.52e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 3198          |\n",
      "|    iterations            | 3             |\n",
      "|    time_elapsed          | 1             |\n",
      "|    total_timesteps       | 6144          |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00020750059 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.47         |\n",
      "|    explained_variance    | 0.00554       |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.42e+04      |\n",
      "|    n_updates             | 20            |\n",
      "|    policy_gradient_loss  | -0.00128      |\n",
      "|    value_loss            | 5.5e+04       |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 49            |\n",
      "|    total_trigger_count   | 513           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.51e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 3067          |\n",
      "|    iterations            | 4             |\n",
      "|    time_elapsed          | 2             |\n",
      "|    total_timesteps       | 8192          |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 7.1019924e-05 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.47         |\n",
      "|    explained_variance    | 0.0334        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.75e+04      |\n",
      "|    n_updates             | 30            |\n",
      "|    policy_gradient_loss  | -0.000676     |\n",
      "|    value_loss            | 5.62e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 68            |\n",
      "|    total_trigger_count   | 466           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.52e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2986          |\n",
      "|    iterations            | 5             |\n",
      "|    time_elapsed          | 3             |\n",
      "|    total_timesteps       | 10240         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00016949928 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0444        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.59e+04      |\n",
      "|    n_updates             | 40            |\n",
      "|    policy_gradient_loss  | -0.000977     |\n",
      "|    value_loss            | 5.43e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 51            |\n",
      "|    total_trigger_count   | 516           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.53e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2873          |\n",
      "|    iterations            | 6             |\n",
      "|    time_elapsed          | 4             |\n",
      "|    total_timesteps       | 12288         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00010986827 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0614        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.38e+04      |\n",
      "|    n_updates             | 50            |\n",
      "|    policy_gradient_loss  | -0.000637     |\n",
      "|    value_loss            | 5.39e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 42            |\n",
      "|    total_trigger_count   | 505           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.55e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2872          |\n",
      "|    iterations            | 7             |\n",
      "|    time_elapsed          | 4             |\n",
      "|    total_timesteps       | 14336         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00017173792 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0717        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.36e+04      |\n",
      "|    n_updates             | 60            |\n",
      "|    policy_gradient_loss  | -0.000935     |\n",
      "|    value_loss            | 5.49e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 35            |\n",
      "|    total_trigger_count   | 476           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.57e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2813          |\n",
      "|    iterations            | 8             |\n",
      "|    time_elapsed          | 5             |\n",
      "|    total_timesteps       | 16384         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00027220766 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0851        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.96e+04      |\n",
      "|    n_updates             | 70            |\n",
      "|    policy_gradient_loss  | -0.00113      |\n",
      "|    value_loss            | 5.65e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 40            |\n",
      "|    total_trigger_count   | 497           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.57e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2756          |\n",
      "|    iterations            | 9             |\n",
      "|    time_elapsed          | 6             |\n",
      "|    total_timesteps       | 18432         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00018818231 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0474        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.49e+04      |\n",
      "|    n_updates             | 80            |\n",
      "|    policy_gradient_loss  | -0.000689     |\n",
      "|    value_loss            | 5.7e+04       |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 39            |\n",
      "|    total_trigger_count   | 501           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.57e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2732          |\n",
      "|    iterations            | 10            |\n",
      "|    time_elapsed          | 7             |\n",
      "|    total_timesteps       | 20480         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00024662155 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0402        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.71e+04      |\n",
      "|    n_updates             | 90            |\n",
      "|    policy_gradient_loss  | -0.00123      |\n",
      "|    value_loss            | 5.44e+04      |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| custom/                  |              |\n",
      "|    time_to_reach_epsilon | 68           |\n",
      "|    total_trigger_count   | 502          |\n",
      "| rollout/                 |              |\n",
      "|    ep_len_mean           | 200          |\n",
      "|    ep_rew_mean           | 2.57e+03     |\n",
      "| time/                    |              |\n",
      "|    fps                   | 2694         |\n",
      "|    iterations            | 11           |\n",
      "|    time_elapsed          | 8            |\n",
      "|    total_timesteps       | 22528        |\n",
      "| train/                   |              |\n",
      "|    approx_kl             | 0.0001203361 |\n",
      "|    clip_fraction         | 0            |\n",
      "|    clip_range            | 0.2          |\n",
      "|    entropy_loss          | -3.46        |\n",
      "|    explained_variance    | 0.0623       |\n",
      "|    learning_rate         | 0.0003       |\n",
      "|    loss                  | 2.87e+04     |\n",
      "|    n_updates             | 100          |\n",
      "|    policy_gradient_loss  | -0.000803    |\n",
      "|    value_loss            | 5.25e+04     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| custom/                  |              |\n",
      "|    time_to_reach_epsilon | 58           |\n",
      "|    total_trigger_count   | 447          |\n",
      "| rollout/                 |              |\n",
      "|    ep_len_mean           | 200          |\n",
      "|    ep_rew_mean           | 2.59e+03     |\n",
      "| time/                    |              |\n",
      "|    fps                   | 2664         |\n",
      "|    iterations            | 12           |\n",
      "|    time_elapsed          | 9            |\n",
      "|    total_timesteps       | 24576        |\n",
      "| train/                   |              |\n",
      "|    approx_kl             | 0.0002389524 |\n",
      "|    clip_fraction         | 0            |\n",
      "|    clip_range            | 0.2          |\n",
      "|    entropy_loss          | -3.46        |\n",
      "|    explained_variance    | 0.0789       |\n",
      "|    learning_rate         | 0.0003       |\n",
      "|    loss                  | 2.7e+04      |\n",
      "|    n_updates             | 110          |\n",
      "|    policy_gradient_loss  | -0.00166     |\n",
      "|    value_loss            | 5.34e+04     |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 41            |\n",
      "|    total_trigger_count   | 469           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.58e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2657          |\n",
      "|    iterations            | 13            |\n",
      "|    time_elapsed          | 10            |\n",
      "|    total_timesteps       | 26624         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00026238593 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0756        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.64e+04      |\n",
      "|    n_updates             | 120           |\n",
      "|    policy_gradient_loss  | -0.0011       |\n",
      "|    value_loss            | 5.45e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 55            |\n",
      "|    total_trigger_count   | 460           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.59e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2630          |\n",
      "|    iterations            | 14            |\n",
      "|    time_elapsed          | 10            |\n",
      "|    total_timesteps       | 28672         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00055198395 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0691        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.53e+04      |\n",
      "|    n_updates             | 130           |\n",
      "|    policy_gradient_loss  | -0.00151      |\n",
      "|    value_loss            | 5.16e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 38            |\n",
      "|    total_trigger_count   | 492           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.59e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2633          |\n",
      "|    iterations            | 15            |\n",
      "|    time_elapsed          | 11            |\n",
      "|    total_timesteps       | 30720         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 9.6019416e-05 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0721        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.48e+04      |\n",
      "|    n_updates             | 140           |\n",
      "|    policy_gradient_loss  | -0.000476     |\n",
      "|    value_loss            | 5.15e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 41            |\n",
      "|    total_trigger_count   | 490           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.58e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2615          |\n",
      "|    iterations            | 16            |\n",
      "|    time_elapsed          | 12            |\n",
      "|    total_timesteps       | 32768         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00012342769 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.101         |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.38e+04      |\n",
      "|    n_updates             | 150           |\n",
      "|    policy_gradient_loss  | -0.000754     |\n",
      "|    value_loss            | 5.08e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 46            |\n",
      "|    total_trigger_count   | 505           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.58e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2626          |\n",
      "|    iterations            | 17            |\n",
      "|    time_elapsed          | 13            |\n",
      "|    total_timesteps       | 34816         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00034768324 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0855        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.45e+04      |\n",
      "|    n_updates             | 160           |\n",
      "|    policy_gradient_loss  | -0.00124      |\n",
      "|    value_loss            | 5.05e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 44            |\n",
      "|    total_trigger_count   | 474           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.58e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2633          |\n",
      "|    iterations            | 18            |\n",
      "|    time_elapsed          | 13            |\n",
      "|    total_timesteps       | 36864         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00019526927 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.134         |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.72e+04      |\n",
      "|    n_updates             | 170           |\n",
      "|    policy_gradient_loss  | -0.000843     |\n",
      "|    value_loss            | 5.21e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 44            |\n",
      "|    total_trigger_count   | 470           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.57e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2641          |\n",
      "|    iterations            | 19            |\n",
      "|    time_elapsed          | 14            |\n",
      "|    total_timesteps       | 38912         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00033118308 |\n",
      "|    clip_fraction         | 0.000195      |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.46         |\n",
      "|    explained_variance    | 0.0966        |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.23e+04      |\n",
      "|    n_updates             | 180           |\n",
      "|    policy_gradient_loss  | -0.00169      |\n",
      "|    value_loss            | 5.02e+04      |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| custom/                  |              |\n",
      "|    time_to_reach_epsilon | 58           |\n",
      "|    total_trigger_count   | 459          |\n",
      "| rollout/                 |              |\n",
      "|    ep_len_mean           | 200          |\n",
      "|    ep_rew_mean           | 2.58e+03     |\n",
      "| time/                    |              |\n",
      "|    fps                   | 2652         |\n",
      "|    iterations            | 20           |\n",
      "|    time_elapsed          | 15           |\n",
      "|    total_timesteps       | 40960        |\n",
      "| train/                   |              |\n",
      "|    approx_kl             | 0.0003026122 |\n",
      "|    clip_fraction         | 0            |\n",
      "|    clip_range            | 0.2          |\n",
      "|    entropy_loss          | -3.45        |\n",
      "|    explained_variance    | 0.115        |\n",
      "|    learning_rate         | 0.0003       |\n",
      "|    loss                  | 2.69e+04     |\n",
      "|    n_updates             | 190          |\n",
      "|    policy_gradient_loss  | -0.001       |\n",
      "|    value_loss            | 5.04e+04     |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 66            |\n",
      "|    total_trigger_count   | 469           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.59e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2663          |\n",
      "|    iterations            | 21            |\n",
      "|    time_elapsed          | 16            |\n",
      "|    total_timesteps       | 43008         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00026853342 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.45         |\n",
      "|    explained_variance    | 0.144         |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.83e+04      |\n",
      "|    n_updates             | 200           |\n",
      "|    policy_gradient_loss  | -0.00114      |\n",
      "|    value_loss            | 5.07e+04      |\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 45            |\n",
      "|    total_trigger_count   | 442           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.59e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2675          |\n",
      "|    iterations            | 22            |\n",
      "|    time_elapsed          | 16            |\n",
      "|    total_timesteps       | 45056         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00021239865 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.45         |\n",
      "|    explained_variance    | 0.131         |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.77e+04      |\n",
      "|    n_updates             | 210           |\n",
      "|    policy_gradient_loss  | -0.000854     |\n",
      "|    value_loss            | 5.11e+04      |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| custom/                  |              |\n",
      "|    time_to_reach_epsilon | 40           |\n",
      "|    total_trigger_count   | 459          |\n",
      "| rollout/                 |              |\n",
      "|    ep_len_mean           | 200          |\n",
      "|    ep_rew_mean           | 2.6e+03      |\n",
      "| time/                    |              |\n",
      "|    fps                   | 2683         |\n",
      "|    iterations            | 23           |\n",
      "|    time_elapsed          | 17           |\n",
      "|    total_timesteps       | 47104        |\n",
      "| train/                   |              |\n",
      "|    approx_kl             | 0.0006738659 |\n",
      "|    clip_fraction         | 0            |\n",
      "|    clip_range            | 0.2          |\n",
      "|    entropy_loss          | -3.45        |\n",
      "|    explained_variance    | 0.111        |\n",
      "|    learning_rate         | 0.0003       |\n",
      "|    loss                  | 2.38e+04     |\n",
      "|    n_updates             | 220          |\n",
      "|    policy_gradient_loss  | -0.00178     |\n",
      "|    value_loss            | 5.04e+04     |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| custom/                  |              |\n",
      "|    time_to_reach_epsilon | 43           |\n",
      "|    total_trigger_count   | 465          |\n",
      "| rollout/                 |              |\n",
      "|    ep_len_mean           | 200          |\n",
      "|    ep_rew_mean           | 2.62e+03     |\n",
      "| time/                    |              |\n",
      "|    fps                   | 2693         |\n",
      "|    iterations            | 24           |\n",
      "|    time_elapsed          | 18           |\n",
      "|    total_timesteps       | 49152        |\n",
      "| train/                   |              |\n",
      "|    approx_kl             | 0.0003272587 |\n",
      "|    clip_fraction         | 0            |\n",
      "|    clip_range            | 0.2          |\n",
      "|    entropy_loss          | -3.45        |\n",
      "|    explained_variance    | 0.167        |\n",
      "|    learning_rate         | 0.0003       |\n",
      "|    loss                  | 2.3e+04      |\n",
      "|    n_updates             | 230          |\n",
      "|    policy_gradient_loss  | -0.00124     |\n",
      "|    value_loss            | 5.11e+04     |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| custom/                  |               |\n",
      "|    time_to_reach_epsilon | 99            |\n",
      "|    total_trigger_count   | 457           |\n",
      "| rollout/                 |               |\n",
      "|    ep_len_mean           | 200           |\n",
      "|    ep_rew_mean           | 2.62e+03      |\n",
      "| time/                    |               |\n",
      "|    fps                   | 2703          |\n",
      "|    iterations            | 25            |\n",
      "|    time_elapsed          | 18            |\n",
      "|    total_timesteps       | 51200         |\n",
      "| train/                   |               |\n",
      "|    approx_kl             | 0.00024533566 |\n",
      "|    clip_fraction         | 0             |\n",
      "|    clip_range            | 0.2           |\n",
      "|    entropy_loss          | -3.45         |\n",
      "|    explained_variance    | 0.133         |\n",
      "|    learning_rate         | 0.0003        |\n",
      "|    loss                  | 2.53e+04      |\n",
      "|    n_updates             | 240           |\n",
      "|    policy_gradient_loss  | -0.00102      |\n",
      "|    value_loss            | 5.07e+04      |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m trigger_count_callback \u001b[38;5;241m=\u001b[39m TriggerCountCallback()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 开始训练模型，设置训练的时间步数\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigger_count_callback\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 根据需要更改时间步数\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 保存训练好的模型\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_consensus_d_f\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/policies.py:645\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m    647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/policies.py:671\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: Optional[BaseFeaturesExtractor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[th\u001b[38;5;241m.\u001b[39mTensor, Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    663\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m        features for the actor and the features for the critic.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshare_features_extractor\u001b[49m:\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features_extractor)\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import tensorflow as tf\n",
    "from env2 import Consensus_D_F  # 从env1.py导入自定义环境\n",
    "\n",
    "# 确保环境符合Gym的规范\n",
    "env = Consensus_D_F(num_agents=5, num_iterations=200, dt=0.1)\n",
    "# 使用稳定性检查器确保环境符合要求\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)\n",
    "\n",
    "# 创建环境实例\n",
    "env = Consensus_D_F()\n",
    "\n",
    "# 定义PPO模型，您可以根据需求调整超参数\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # 使用多层感知器的策略网络\n",
    "    env,            # 自定义环境\n",
    "    verbose=1,      # 输出训练信息\n",
    "    tensorboard_log=\"./ppo_consensus_d_f_tensorboard1/\"  # 日志存储路径（可选）\n",
    ")\n",
    "\n",
    "class TriggerCountCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TriggerCountCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        # 检查是否是 episode 的最后一步\n",
    "        if self.locals[\"dones\"][0]:  # 当 episode 结束时\n",
    "            total_trigger_count = self.training_env.get_attr(\"total\")[0]\n",
    "            time_to_reach_epsilon = self.training_env.get_attr(\"time\")[0]\n",
    "            \n",
    "            # 记录到 TensorBoard\n",
    "            self.logger.record(\"custom/total_trigger_count\", total_trigger_count)\n",
    "            if time_to_reach_epsilon is not None:\n",
    "                self.logger.record(\"custom/time_to_reach_epsilon\", time_to_reach_epsilon)\n",
    "        return True\n",
    "\n",
    "trigger_count_callback = TriggerCountCallback()\n",
    "\n",
    "# 开始训练模型，设置训练的时间步数\n",
    "model.learn(total_timesteps=5000000, callback=trigger_count_callback)  # 根据需要更改时间步数\n",
    "\n",
    "# 保存训练好的模型\n",
    "model.save(\"ppo_consensus_d_f\")\n",
    "\n",
    "# # 加载模型并测试\n",
    "# model = PPO.load(\"ppo_consensus_d_f\", env=env)\n",
    "\n",
    "# # 测试模型，运行一个episode\n",
    "# # 测试模型，运行一个 episode\n",
    "# obs, _ = env.reset()  # 只获取 obs，忽略 info\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, _, info = env.step(action)  # 忽略 `truncated`，只解包前四个值\n",
    "#     #env.render()  # 若环境中有 render 函数，可以进行渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from env4 import Consensus_D_F  # 从env1.py导入自定义环境\n",
    "\n",
    "# 确保环境符合Gym的规范\n",
    "env = Consensus_D_F(num_agents=5, num_iterations=200, dt=0.1)\n",
    "check_env(env)\n",
    "\n",
    "# 创建训练环境实例\n",
    "train_env = Consensus_D_F()\n",
    "\n",
    "# 创建评估环境实例\n",
    "eval_env = Consensus_D_F()\n",
    "\n",
    "# 定义PPO模型，您可以根据需求调整超参数\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",    # 使用多层感知器的策略网络\n",
    "    train_env,      # 自定义环境\n",
    "    verbose=1,      # 输出训练信息\n",
    "    #tensorboard_log=\"./ppo_consensus_d_f_tensorboard/\"  # 日志存储路径（可选）\n",
    "    tensorboard_log=\"./ppo_consensus_d_f_tensorboard2/\"  # 日志存储路径（可选）\n",
    ")\n",
    "\n",
    "class TriggerCountCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TriggerCountCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        # 检查是否是 episode 的最后一步\n",
    "        if self.locals[\"dones\"][0]:  # 当 episode 结束时\n",
    "            total_trigger_count = self.training_env.get_attr(\"total\")[0]\n",
    "            time_to_reach_epsilon = self.training_env.get_attr(\"time\")[0]\n",
    "            success = self.training_env.get_attr(\"s\")[0]\n",
    "            \n",
    "            # 记录到 TensorBoard\n",
    "            self.logger.record(\"custom/total_trigger_count\", total_trigger_count)\n",
    "            self.logger.record(\"custom/success\", success)\n",
    "            if time_to_reach_epsilon is not None:\n",
    "                self.logger.record(\"custom/time_to_reach_epsilon\", time_to_reach_epsilon)\n",
    "        return True\n",
    "\n",
    "# 创建评估回调，用于每隔 500000 步进行评估，并将最优模型保存在 `best_model` 文件夹中\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,          # 每 500000 步进行一次评估\n",
    "    n_eval_episodes=20,        # 每次评估进行 20 个 episode\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# 创建自定义的触发计数回调\n",
    "trigger_count_callback = TriggerCountCallback()\n",
    "\n",
    "# 开始训练模型，设置训练的时间步数，带有评估回调\n",
    "#model.learn(total_timesteps=15000000, callback=[trigger_count_callback, eval_callback])\n",
    "model.learn(total_timesteps=5000000, callback=[trigger_count_callback])\n",
    "\n",
    "# 保存最终训练好的模型\n",
    "model.save(\"ppo_consensus_d_f\")\n",
    "\n",
    "# 如果需要加载并测试模型\n",
    "# model = PPO.load(\"ppo_consensus_d_f\", env=env)\n",
    "# obs, _ = env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, _, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估回调，用于每隔 500000 步进行评估，并将最优模型保存在 `best_model` 文件夹中\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,          # 每 500000 步进行一次评估\n",
    "    n_eval_episodes=20,        # 每次评估进行 20 个 episode\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "# 开始训练模型，设置训练的时间步数，带有评估回调\n",
    "model.learn(total_timesteps=5000000, callback=[trigger_count_callback, eval_callback])\n",
    "\n",
    "# 保存最终训练好的模型\n",
    "model.save(\"ppo_consensus_d_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laplacian_matrix = np.array([\n",
      "    [ 1,  0, -1,  0,  0],\n",
      "    [ 0,  1,  0,  0, -1],\n",
      "    [-1,  0,  2, -1,  0],\n",
      "    [ 0,  0, -1,  1,  0],\n",
      "    [ 0, -1,  0,  0,  1],\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_laplacian(n, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    随机生成一个 n 维的拉普拉斯矩阵。\n",
    "    \n",
    "    参数:\n",
    "        n (int): 拉普拉斯矩阵的维度，即节点数量。\n",
    "        edge_probability (float): 边的生成概率，默认值为 0.3。\n",
    "    \n",
    "    返回:\n",
    "        np.ndarray: n 维的拉普拉斯矩阵。\n",
    "    \"\"\"\n",
    "    # 初始化邻接矩阵（对称矩阵）\n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    # 遍历上三角矩阵部分，随机生成边\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if np.random.rand() < edge_probability:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1  # 保持对称性\n",
    "\n",
    "    # 计算度矩阵\n",
    "    degree_matrix = np.diag(adjacency_matrix.sum(axis=1))\n",
    "    \n",
    "    # 计算拉普拉斯矩阵：L = D - A\n",
    "    laplacian_matrix = degree_matrix - adjacency_matrix\n",
    "    \n",
    "    return laplacian_matrix\n",
    "\n",
    "def print_laplacian_matrix(laplacian_matrix):\n",
    "    \"\"\"\n",
    "    以格式化的方式打印拉普拉斯矩阵，使其便于复制。\n",
    "    \"\"\"\n",
    "    print(\"laplacian_matrix = np.array([\")\n",
    "    for row in laplacian_matrix:\n",
    "        print(\"    [\" + \", \".join(f\"{int(elem):2}\" for elem in row) + \"],\")\n",
    "    print(\"])\")\n",
    "\n",
    "# 示例\n",
    "n = 5  # 设置矩阵维度\n",
    "laplacian_matrix = generate_random_laplacian(n)\n",
    "#print(\"随机生成的拉普拉斯矩阵：\")\n",
    "print_laplacian_matrix(laplacian_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyj/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:271: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Action spaces do not match: Discrete(32) != MultiBinary(5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m Consensus_D_F(num_agents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     10\u001b[0m check_env(env)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo_consensus_d_f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#model = PPO.load(\"./best_model/best_model.zip\", env=env)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model = DQN.load(\"dqn_consensus_d_f\", env=env)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 初始位置列表\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#initial_positions = [0.1, -0.7, 0.1, -0.4, 0.5]  # 初始位置列表\u001b[39;00m\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:716\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/stable_baselines3/common/utils.py:233\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Action spaces do not match: Discrete(32) != MultiBinary(5)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from env3 import Consensus_D_F  # 从env1.py导入自定义环境# 加载预训练模型\n",
    "\n",
    "# 确保环境符合Gym的规范\n",
    "env = Consensus_D_F(num_agents=5, num_iterations=200, dt=0.1)\n",
    "check_env(env)\n",
    "model = PPO.load(\"ppo_consensus_d_f\", env=env)\n",
    "#model = PPO.load(\"./best_model/best_model.zip\", env=env)\n",
    "\n",
    "# model = DQN.load(\"dqn_consensus_d_f\", env=env)\n",
    "\n",
    "# 初始位置列表\n",
    "#initial_positions = [0.1, -0.7, 0.1, -0.4, 0.5]  # 初始位置列表\n",
    "env.reset()\n",
    "\n",
    "# 获取200步的动作矩阵\n",
    "actions_over_time = env.render(model)\n",
    "for action in actions_over_time:\n",
    "    print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
